{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tfidf.ipynb","provenance":[],"mount_file_id":"15Qb9q0l3IE5YUwRa2cM59jXoFSIPgTDB","authorship_tag":"ABX9TyNKGVXUEafogje5ZDmPdH0k"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"uxUkzlE_6irL"},"source":["import pandas as pd\n","import sklearn as sk\n","import math \n","import nltk\n","import os\n","import fnmatch\n","import random\n","import numpy as np\n","from xml.etree import ElementTree\n","import re\n","import string\n","import spacy\n","from nltk.corpus import stopwords          \n","from nltk.stem import PorterStemmer        \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","from bs4 import BeautifulSoup as bs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWqAxHdM6zI3","executionInfo":{"status":"ok","timestamp":1602649821625,"user_tz":-360,"elapsed":3196,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"abcc7b3f-6448-46ec-c3a5-d012f40d1299","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# download the stopwords from NLTK\n","#nltk.download('all')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"k9bmr9u3EhjO"},"source":["def makeTextLower(text):\n","  result = text.lower() \n","  return result "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gccnY93EEowm"},"source":["def MakeNumberRemove(text): \n","  #result = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', text)\n","  result = re.sub(r'[-+]?\\d+', '', text) \n","  return result "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXvVsv0cEr5_"},"source":["def MakeHTMLremove(text):\n","  '''\n","  result = re.compile('(<.*?>)')\n","  result = result.sub(r'', text) \n","  '''\n","  cleancode = re.compile('<code>.*?</code>')\n","  cleanr = re.compile('<.*?>')\n","  cleanentity = re.compile('&.*;')\n","  cleantext = re.sub(cleancode, '', text)\n","  cleantext = re.sub(cleanr, ' ', cleantext)\n","  cleantext = re.sub(cleanentity, ' ', cleantext)\n","  \n","  return cleantext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ELJRi0BOEuNw"},"source":["def makeRemoveExtraApace(text):\n","  result = \" \".join(text.split())  \n","  return result   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZB6gdVDExDy"},"source":["def makeRemoveHyperLink(text):\n","  result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0h_ThUBEzk1"},"source":["def makeRemovePunctuation(text):\n","  \n","  #print(string.punctuation)\n","  #translator = str.maketrans('', '', string.punctuation)\n","  result = text=text.translate((str.maketrans('','',string.punctuation)))\n","  return result\n","  \n","  '''\n","  puncts = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","\n","  for punct in puncts:\n","    if punct in text:\n","      text = text.replace(punct, '')\n","\n","  return text\n","  '''\n","#makeRemovePunctuation(\"Hi;\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIagPNV1E4I1"},"source":["def makeWordLemmatize(text):\n","  lemmatizer = WordNetLemmatizer()\n","  #word_tokens = word_tokenize(text)\n","  lemmas = [lemmatizer.lemmatize(word,pos='v') for word in text] \n","  return lemmas "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yetF8QwBSpsf"},"source":["def cleanTextData(text):\n","\n","  #uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n","  text = text.lower()\n","  # specific\n","  text = re.sub(r\"won't\", \"will not\", text)\n","  text = re.sub(r\"can\\'t\", \"can not\", text)\n","\n","  # general\n","  text = re.sub(r\"n\\'t\", \" not\", text)\n","  text = re.sub(r\"\\'re\", \" are\", text)\n","  text = re.sub(r\"\\'s\", \" is\", text)\n","  text = re.sub(r\"\\'d\", \" would\", text)\n","  text = re.sub(r\"\\'ll\", \" will\", text)\n","  text = re.sub(r\"\\'t\", \" not\", text)\n","  text = re.sub(r\"\\'ve\", \" have\", text)\n","  text = re.sub(r\"\\'m\", \" am\", text)\n","  text = re.sub(r\"\\n\", \"\", text)\n","  #text = re.sub(uri_re, \"\", text)\n","  text = re.sub(r\"what's\", \"what is \", text)\n","  text = re.sub(r\"\\'s\", \" \", text)\n","  text = re.sub(r\"\\'ve\", \" have \", text)\n","  text = re.sub(r\"can't\", \"can not \", text)\n","  text = re.sub(r\"n't\", \" not \", text)\n","  text = re.sub(r\"i'm\", \"i am \", text)\n","  text = re.sub(r\"\\'re\", \" are \", text)\n","  text = re.sub(r\"\\'d\", \" would \", text)\n","  text = re.sub(r\"\\'ll\", \" will \", text)\n","  text = re.sub(r\"\\'scuse\", \" excuse \", text)\n","  text = re.sub(r\"\\'\\n\", \" \", text)\n","  text = re.sub(r\"\\'\\xa0\", \" \", text)\n","  text = re.sub('\\s+', ' ', text)\n","  text = text.strip(' ')\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MM6hQ8WE7n6"},"source":["def removeStopWords(text):\n","  stopwords_english = stopwords.words('english')\n","  texts_clean = []\n","  #print(text)\n","  text = word_tokenize(text)\n","  for word in text: \n","    if word not in stopwords_english:  \n","        texts_clean.append(word)\n","  return texts_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzPbn_m_aIOV"},"source":["sp = spacy.load('en_core_web_sm')\n","spacy_stopwords = sp.Defaults.stop_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_wt1Jn5YiJn"},"source":["def removeSpacyStopWords(text,spacy_stopwords):\n","  \n","  texts_clean = []\n","  #print(text)\n","  for word in text: \n","    if word not in spacy_stopwords:  \n","        texts_clean.append(word)\n","  return texts_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAX1KGfwdF6z"},"source":["def makeStemming(text):\n","  ps = PorterStemmer() \n","  #words = word_tokenize(text)\n","  lemmas = [ps.stem(word) for word in text] \n","  return lemmas  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wCzRBOSE-s5"},"source":["def preprocessText(text):\n","\n","  '''\n","  - html remove\n","  - Transforming abbreviations\n","  - Removing punctuation\n","  - Lemmatizing/stemming words ?\n","  - Removing stop words\n","  '''\n","  result=re.sub(r\"<a.*</a>\", '', text)\n","  result=re.sub(r\"<.+?>\",'',result)\n","  #result = makeTextLower(text)\n","  result=re.sub(r'[^\\x00-\\x7F]+','',result)\n","  result = MakeHTMLremove(result)\n","  #result = makeRemoveExtraApace(result)\n","  result = makeTextLower(result)\n","  \n","  #result = cleanTextData(result)\n","  result = MakeNumberRemove(result)\n","  result = makeRemoveHyperLink(result)\n","  result = makeRemovePunctuation(result)\n","  \n","  result = removeStopWords(result)\n","  result = removeSpacyStopWords(result,spacy_stopwords)\n","  result = makeWordLemmatize(result)\n","  result = makeStemming(result)\n","  \n","  \n","  #result = makeTextLower(result)\n","\n","  return result  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"52LPpq1E7GWC","executionInfo":{"status":"ok","timestamp":1602649831964,"user_tz":-360,"elapsed":13397,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"e71c19ab-d04a-4bfe-a6fd-deab8fac7a89","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["train_path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"\n","#topic name reading\n","topic_number =0\n","topic_list = {}\n","with open(\"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/topics.txt\") as fp:\n","  lines = fp.readlines() \n","  for line in lines:\n","    line = line.strip()\n","    topic_list[line] = topic_number\n","    topic_number += 1\n","    \n","    if topic_number == 3 :\n","      break\n","    \n","fp.close()\n","print(topic_list)\n","topicLength = len(topic_list)\n","#dom = ElementTree.parse(train_path+xml_file[0])\n","#topic_list = {\"Anime\":0,\"Arduino\":1,\"Biology\":2,\"Chess\":3}\n","#topicLength = len(topic_list)\n","#print(topicLength)\n","#each topic data size\n","\n","# train data and validation data \n","topicDataSize = 700\n","trainDataSize = 500\n","validationDataSize = 200\n"," \n","position_array = []\n","for i in range(0,topicDataSize):\n","  position_array.append(i)\n","#print(position_array)\n","#random.seed(20)\n","#random.shuffle(position_array)\n","#print(position_array)\n","\n","datastore = {}\n","validstore = {}\n","\n","#data collection\n","for topic in topic_list:\n","  with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","    content = file.read()\n","    soup = bs(content)\n","    rowList = soup.findAll(\"row\")\n","    \n","    #train data\n","    for i in range(0,trainDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      datastore[topicPair] = np.array(word_list)\n","    \n","    #validation data\n","    for i in range(trainDataSize,trainDataSize+validationDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      validstore[topicPair] = np.array(word_list)\n","'''\n","for i in validdataStore:\n","  print(i)\n","  print(validdataStore[i])\n","'''      "],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'Coffee': 0, 'Arduino': 1, 'Anime': 2}\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfor i in validdataStore:\\n  print(i)\\n  print(validdataStore[i])\\n'"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"9jbDIu54vTi5"},"source":["def computeTF(wordDict, doc,uniqueWord):\n","  tfDict = {}\n","  corpusCount = len(doc)\n","\n","  for word, count in zip(uniqueWord,wordDict):\n","      tfDict[word] = count/float(corpusCount)\n","  return tfDict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KES4h7XwvWw7"},"source":["def numberDocsForWord(datastore,words):\n","  wordVal = {}\n","  value = list(datastore.values())\n","\n","  for word in words:\n","    count = sum(word in e for e in value)    \n","    wordVal[word] = count\n","  \n","  return wordVal\n","\n","def uniqueWordFunc(trainDataStore,testStore):\n","  uniqueWord = []\n","  for pair in trainDataStore:\n","    wordList = trainDataStore[pair]\n","    for word in wordList:\n","      if word not in uniqueWord:\n","        uniqueWord.append(word)\n","  wordval = numberDocsForWord(datastore,uniqueWord)\n","  for pair in testStore:\n","    wordList = testStore[pair]\n","    for word in wordList:\n","      if word not in uniqueWord:\n","        uniqueWord.append(word)\n","  return uniqueWord,wordval\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pi6UgVU58Vpx","executionInfo":{"status":"ok","timestamp":1602649915209,"user_tz":-360,"elapsed":96563,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"e6caa24b-5043-4d76-9211-6ae4f19188b6","colab":{"base_uri":"https://localhost:8080/","height":103}},"source":["def countData(datastore,validstore,uniqueWord):\n","\n","  countDatastore = {}\n","  countDatastoreTest = {}\n","  \n","  #corpusCount = len(datastore)\n","  #print(uniqueWord)\n","  for key in datastore:\n","    tempArray = list(datastore[key])\n","    wordDictA = []\n","    for word in uniqueWord:\n","      wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","    countDatastore[key] = wordDictA\n","    \n","  \n","  for key in validstore:\n","    tempArray = list(validstore[key])\n","    wordDictA = []\n","    for word in uniqueWord:\n","      wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","    countDatastoreTest[key] = wordDictA\n","  \n","  return countDatastore,countDatastoreTest\n","\n","uniqueWord,wordval = uniqueWordFunc(datastore,validstore)    \n","tfData,tfDataTest = countData(datastore,validstore,uniqueWord)\n","print(len(uniqueWord))\n","'''\n","def countTF(countDatastore,countDatastoreTest):\n","  tfData = {}\n","  tfDataTest = {}\n","  for key in countDatastore:\n","    val = computeTF(countDatastore[key], datastore[key],uniqueWord)\n","    tfData[key] = val\n","  \n","  for key in countDatastoreTest:\n","    val = computeTF(countDatastoreTest[key], validstore[key],uniqueWord)\n","    tfDataTest[key] = val\n","  \n","  return tfData,tfDataTest\n","\n","tfData,tfDataTest = countTF(countDatastore,countDatastoreTest)\n","'''\n","#del countDatastore\n","#del countDatastoreTest\n","#print(countDatastore)\n","#print(datastore)\n","#print(tfData)\n","#print(tfDataTest)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10810\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef countTF(countDatastore,countDatastoreTest):\\n  tfData = {}\\n  tfDataTest = {}\\n  for key in countDatastore:\\n    val = computeTF(countDatastore[key], datastore[key],uniqueWord)\\n    tfData[key] = val\\n  \\n  for key in countDatastoreTest:\\n    val = computeTF(countDatastoreTest[key], validstore[key],uniqueWord)\\n    tfDataTest[key] = val\\n  \\n  return tfData,tfDataTest\\n\\ntfData,tfDataTest = countTF(countDatastore,countDatastoreTest)\\n'"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"f7Xq62MJGru7","executionInfo":{"status":"ok","timestamp":1602650010408,"user_tz":-360,"elapsed":191702,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"0143a6ab-7e21-459d-8c95-4efe0ccac995","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["def computeIDF(datastore,validstore,numberofDoc,dictionaryWords,wordval,tfData,tfDataTest):\n","  \n","  idfDict = {}\n","  idfDict = dict.fromkeys(set(dictionaryWords), 0)\n","  #wordval = numberDocsForWord(datastore,dictionaryWords)\n","  #print(wordval)\n","  for key in datastore:\n","    tempArray = datastore[key]\n","    temp = []\n","    for word in dictionaryWords:\n","      if word in tempArray:\n","        temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","      else :\n","        temp.append(0.0)\n","    tfData[key] = list(np.multiply(temp,tfData[key]))\n","\n","  for key in validstore:\n","    tempArray = validstore[key]\n","    temp = []\n","    for word in dictionaryWords:\n","      if word in tempArray and word in wordval:\n","        temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","      else :\n","        temp.append(0.0)\n","    tfDataTest[key] = list(np.multiply(temp,tfDataTest[key]))\n","      \n","  return tfData,tfDataTest\n","\n","numberofDoc = len(datastore)\n","#print(numberofDoc)\n","\n","idfData = {}\n","#wordval = numberDocsForWord(datastore,uniqueWord)\n","\n","print(\"doneOne\")\n","tfidfDataArray,tfidfDataTestArray = computeIDF(datastore,validstore,numberofDoc,uniqueWord,wordval,tfData,tfDataTest)\n","'''\n","for key in datastore:\n","  idf = computeIDF(datastore[key],numberofDoc,uniqueWord,wordval)\n","  idfData[key] = idf\n","\n","idfDataTest = {}\n","for key in validstore:\n","  idf = computeIDF(validstore[key],numberofDoc,uniqueWord,wordval)\n","  idfDataTest[key] = idf\n","'''\n","#print(idfData)\n","#print(idfDataTest)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["doneOne\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfor key in datastore:\\n  idf = computeIDF(datastore[key],numberofDoc,uniqueWord,wordval)\\n  idfData[key] = idf\\n\\nidfDataTest = {}\\nfor key in validstore:\\n  idf = computeIDF(validstore[key],numberofDoc,uniqueWord,wordval)\\n  idfDataTest[key] = idf\\n'"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"biU-TJFIREMd","executionInfo":{"status":"ok","timestamp":1602650010410,"user_tz":-360,"elapsed":191659,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"d73e24bc-6b13-4844-8a44-e0070dea4510","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["'''\n","def computeTFIDF(tfBow, idfs):\n","  tfidf = {}\n","  for word, val in tfBow.items():\n","      tfidf[word] = val*idfs[word]\n","  return tfidf\n","\n","tfidfData = {}\n","tfidfDataTest = {}\n","\n","for key in idfData:\n","  tfidfData[key] = computeTFIDF(tfData[key],idfData[key])\n","for key in idfDataTest:\n","  tfidfDataTest[key] = computeTFIDF(tfDataTest[key],idfDataTest[key])\n","\n","del tfData\n","del tfDataTest\n","del idfData\n","del idfDataTest\n","#print(tfidfData)\n","#print(tfidfDataTest) \n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef computeTFIDF(tfBow, idfs):\\n  tfidf = {}\\n  for word, val in tfBow.items():\\n      tfidf[word] = val*idfs[word]\\n  return tfidf\\n\\ntfidfData = {}\\ntfidfDataTest = {}\\n\\nfor key in idfData:\\n  tfidfData[key] = computeTFIDF(tfData[key],idfData[key])\\nfor key in idfDataTest:\\n  tfidfDataTest[key] = computeTFIDF(tfDataTest[key],idfDataTest[key])\\n\\ndel tfData\\ndel tfDataTest\\ndel idfData\\ndel idfDataTest\\n#print(tfidfData)\\n#print(tfidfDataTest) \\n'"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"eAzb5PuuSuIP","executionInfo":{"status":"ok","timestamp":1602650010411,"user_tz":-360,"elapsed":191633,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"4a493a01-afc9-4e49-e367-89fe31e053e6","colab":{"base_uri":"https://localhost:8080/","height":120}},"source":["'''\n","def tfidfToArray(tfidfData,tfidfDataTest):\n","  tfidfDataArray = {}\n","  tfidfDataTestArray = {}\n","\n","  for key in tfidfData:\n","    tempArray = []\n","    for val in tfidfData[key]:\n","      tempArray.append(tfidfData[key][val])\n","    tfidfDataArray[key] = tempArray\n","  #print(tfidfDataArray)\n","\n","  for key in tfidfDataTest:\n","    tempArray = []\n","    for val in tfidfDataTest[key]:\n","      tempArray.append(tfidfDataTest[key][val])\n","    tfidfDataTestArray[key] = tempArray\n","  #print(tfidfDataArray)\n","  return tfidfDataArray,tfidfDataTestArray\n","\n","#tfidfDataArray,tfidfDataTestArray=tfidfToArray(tfidfData,tfidfDataTest) \n","#print(tfidfDataArray)\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef tfidfToArray(tfidfData,tfidfDataTest):\\n  tfidfDataArray = {}\\n  tfidfDataTestArray = {}\\n\\n  for key in tfidfData:\\n    tempArray = []\\n    for val in tfidfData[key]:\\n      tempArray.append(tfidfData[key][val])\\n    tfidfDataArray[key] = tempArray\\n  #print(tfidfDataArray)\\n\\n  for key in tfidfDataTest:\\n    tempArray = []\\n    for val in tfidfDataTest[key]:\\n      tempArray.append(tfidfDataTest[key][val])\\n    tfidfDataTestArray[key] = tempArray\\n  #print(tfidfDataArray)\\n  return tfidfDataArray,tfidfDataTestArray\\n\\n#tfidfDataArray,tfidfDataTestArray=tfidfToArray(tfidfData,tfidfDataTest) \\n#print(tfidfDataArray)\\n'"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"WvK7M5OSvNn9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mmDrA8wCBbcb","executionInfo":{"status":"ok","timestamp":1602650010412,"user_tz":-360,"elapsed":191598,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"54311ab2-7d57-41d1-d7d0-0ff2af7d98d7","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["list1 = np.array([1,2,3])\n","list2 = np.array([-2,0,3])\n","\n","np.dot(list1,list2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"AqThAjgCzhJW","executionInfo":{"status":"ok","timestamp":1602650010412,"user_tz":-360,"elapsed":191579,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"d5b7dd1a-a0a8-4f18-f560-e84135a7d73a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["list1 = np.array([1,2])\n","print(np.array(np.sqrt(np.sum((list1)**2))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.23606797749979\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FV_OjHRPVXFE","executionInfo":{"status":"ok","timestamp":1602650010414,"user_tz":-360,"elapsed":191563,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"bf933b1d-faaf-408d-85d1-9518ec1331a6","colab":{"base_uri":"https://localhost:8080/","height":103}},"source":["'''\n","allDistances = []\n","for keyval in tfidfDataTestArray:\n","  for key in tfidfDataArray:\n","    #print(key)\n","    #list1 = np.array(tfidfDataArray[key])\n","    #list2 = np.array(tfidfDataTestArray[keyval])\n","    list1Val = np.sqrt(np.sum(np.square(tfidfDataArray[key])))\n","    list2Val = np.sqrt(np.sum(np.square(tfidfDataTestArray[keyval])))\n","    sim = np.dot(tfidfDataArray[key],tfidfDataTestArray[keyval])/(list1Val*list2Val)\n","    #print(list1)\n","    #print(list2)\n","    allDistances.append((key,sim))\n","\n","  allDistances.sort(key=lambda x: x[1],reverse=True)\n","#print(allDistances)\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nallDistances = []\\nfor keyval in tfidfDataTestArray:\\n  for key in tfidfDataArray:\\n    #print(key)\\n    #list1 = np.array(tfidfDataArray[key])\\n    #list2 = np.array(tfidfDataTestArray[keyval])\\n    list1Val = np.sqrt(np.sum(np.square(tfidfDataArray[key])))\\n    list2Val = np.sqrt(np.sum(np.square(tfidfDataTestArray[keyval])))\\n    sim = np.dot(tfidfDataArray[key],tfidfDataTestArray[keyval])/(list1Val*list2Val)\\n    #print(list1)\\n    #print(list2)\\n    allDistances.append((key,sim))\\n\\n  allDistances.sort(key=lambda x: x[1],reverse=True)\\n#print(allDistances)\\n'"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"id":"Yte714XlgYaI"},"source":["def cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval):\n","  \n","  listVal1 = np.sqrt(np.sum(np.square(testingBinArrCS),axis=1))\n","  listVal2 = np.sqrt(np.sum(np.square(tfidfDataTestArray[keyval])))\n","  dotResult = np.dot(testingBinArrCS,tfidfDataTestArray[keyval])\n","  finalResult = np.divide(dotResult,np.dot(listVal1,listVal2))\n","\n","  finalResult, headerCS = (list(t) for t in zip(*sorted(zip(finalResult, headerCS),reverse=True)))\n","\n","  return headerCS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zxfs6GfxXMkX","executionInfo":{"status":"ok","timestamp":1602650048212,"user_tz":-360,"elapsed":229327,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"204ee8bd-68b1-4de5-f298-4eb071b679d4","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["headerCS = list(tfidfDataArray.keys())\n","testingBinArrCS = np.array(list(tfidfDataArray.values()))\n","\n","allBinDisctance = {}\n","for keyval in tfidfDataTestArray:\n","  predicted = cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval)\n","  allBinDisctance[keyval] = predicted\n","\n","n_neighbors = [1,3,5]\n","for neighbor in n_neighbors:\n","  print(\"Neighbor :\",neighbor)\n","  totalCount = 0\n","  correctCount = 0\n","  for keyval in tfidfDataTestArray:\n","    predArray = allBinDisctance[keyval]\n","    voting_count = {}\n","    for val in predArray[:neighbor]:\n","      if val[0] in voting_count:\n","        voting_count[val[0]] += 1\n","      else :\n","        voting_count[val[0]] = 1\n","\n","    prediction = max(voting_count, key=voting_count.get)\n","    if prediction == keyval[0]:\n","      correctCount += 1\n","    totalCount += 1\n","    \n","  print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Neighbor : 1\n","Total Correct Count:  548  Total Wrong Count:  33  Accuracy:  94.32013769363166\n","Neighbor : 3\n","Total Correct Count:  555  Total Wrong Count:  26  Accuracy:  95.5249569707401\n","Neighbor : 5\n","Total Correct Count:  559  Total Wrong Count:  22  Accuracy:  96.21342512908778\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hFayRDXe09l-","executionInfo":{"status":"ok","timestamp":1602650048213,"user_tz":-360,"elapsed":229293,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"737d5095-0f83-4ef2-dd53-467751e22960","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import pairwise_distances\n","\n","list1=np.array([[1,2]])\n","list2 = np.array([[3,3]])\n","\n","pairwise_distances(list1,list2,metric='hamming')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.]])"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"L5Tmeonvmey7","executionInfo":{"status":"ok","timestamp":1602650048214,"user_tz":-360,"elapsed":229274,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"366d9085-8ad0-4eca-ee82-d283b29f03d6","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","from itertools import chain\n","dictionary = {\"raj\": [1,2], \"striver\": [2,2], \"vikram\": [3,4],\"pranto\":[2,4]} \n","value = list(dictionary.values())\n","sum(1 in e for e in value)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"code","metadata":{"id":"sglRsy7Vyz8y","executionInfo":{"status":"ok","timestamp":1602650048215,"user_tz":-360,"elapsed":229254,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"2495d68b-88af-41e6-fd05-4823cf393a99","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["list1=[1,2]\n","list2 = [2,3]\n","(list(np.multiply(list1,list2)))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 6]"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"TDtk27bxhrV-","outputId":"0f42b823-e51a-496b-eefc-a48007e5d84d","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip3 install bs4\n","!pip install spacy\n","\n","import nltk\n","import math\n","import os\n","import fnmatch\n","import random\n","import numpy as np\n","from xml.etree import ElementTree\n","import re\n","import string\n","import spacy\n","from scipy.spatial import distance\n","from nltk.corpus import stopwords          \n","from nltk.stem import PorterStemmer        \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from bs4 import BeautifulSoup as bs\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","sp = spacy.load('en_core_web_sm')\n","spacy_stopwords = sp.Defaults.stop_words\n","\n","def preprocessText(text):\n","\n","  def makeTextLower(text):\n","    result = text.lower() \n","    return result \n","\n","  def MakeNumberRemove(text): \n","    #result = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', text)\n","    result = re.sub(r'[-+]?\\d+', '', text) \n","    return result \n","\n","  def MakeHTMLremove(text):\n","    cleancode = re.compile('<code>.*?</code>')\n","    cleanr = re.compile('<.*?>')\n","    cleanentity = re.compile('&.*;')\n","    cleantext = re.sub(cleancode, '', text)\n","    cleantext = re.sub(cleanr, ' ', cleantext)\n","    cleantext = re.sub(cleanentity, ' ', cleantext)  \n","    return cleantext\n","\n","  def makeRemoveExtraApace(text):\n","    result = \" \".join(text.split())  \n","    return result\n","\n","  def makeRemoveHyperLink(text):\n","    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","    return result  \n","\n","  def makeRemovePunctuation(text):\n","    result = text=text.translate((str.maketrans('','',string.punctuation)))\n","    return result\n","  \n","  def makeWordLemmatize(text):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = [lemmatizer.lemmatize(word,pos='v') for word in text] \n","    return lemmas\n","\n","  def removeStopWords(text):\n","    stopwords_english = stopwords.words('english')\n","    texts_clean = []\n","    #print(text)\n","    text = word_tokenize(text)\n","    for word in text: \n","      if word not in stopwords_english:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def removeSpacyStopWords(text,spacy_stopwords):\n","    texts_clean = []\n","    #print(text)\n","    for word in text: \n","      if word not in spacy_stopwords:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def makeStemming(text):\n","    ps = PorterStemmer() \n","    #words = word_tokenize(text)\n","    lemmas = [ps.stem(word) for word in text] \n","    return lemmas  \n","\n","  result = text\n","  result=re.sub(r\"<.+?>\",'',result)\n","  result=re.sub(r'[^\\x00-\\x7F]+','',result)\n","  result = MakeHTMLremove(result)\n","  result = makeTextLower(result)\n","  result = MakeNumberRemove(result)\n","  result = makeRemoveHyperLink(result)\n","  result = makeRemovePunctuation(result)\n","  result = removeStopWords(result)\n","  result = removeSpacyStopWords(result,spacy_stopwords)\n","  result = makeWordLemmatize(result)\n","  result = makeStemming(result)\n","  return result\n","\n","def runTFIDF(datastore,validstore,topic_list,topicLength,type):\n","\n","  def numberDocsForWord(datastore,words):\n","    wordVal = {}\n","    value = list(datastore.values())\n","    for word in words:\n","      count = sum(word in e for e in value)    \n","      wordVal[word] = count\n","    return wordVal\n","\n","  def uniqueWordFunc(trainDataStore,testStore):\n","    uniqueWord = []\n","    for pair in trainDataStore:\n","      wordList = trainDataStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    wordval = numberDocsForWord(datastore,uniqueWord)\n","    for pair in testStore:\n","      wordList = testStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    return uniqueWord,wordval\n","\n","  def countData(datastore,validstore,uniqueWord):\n","    countDatastore = {}\n","    countDatastoreTest = {}\n","    \n","    for key in datastore:\n","      tempArray = list(datastore[key])\n","      wordDictA = []\n","      for word in uniqueWord:\n","        wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","      countDatastore[key] = wordDictA\n","      \n","    for key in validstore:\n","      tempArray = list(validstore[key])\n","      wordDictA = []\n","      for word in uniqueWord:\n","        wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","      countDatastoreTest[key] = wordDictA\n","    \n","    return countDatastore,countDatastoreTest\n","\n","  uniqueWord,wordval = uniqueWordFunc(datastore,validstore)    \n","  tfData,tfDataTest = countData(datastore,validstore,uniqueWord)\n","  print(len(uniqueWord))\n","\n","  def computeIDF(datastore,validstore,numberofDoc,dictionaryWords,wordval,tfData,tfDataTest):\n","    \n","    idfDict = {}\n","    idfDict = dict.fromkeys(set(dictionaryWords), 0)\n","    \n","    for key in datastore:\n","      tempArray = datastore[key]\n","      temp = []\n","      for word in dictionaryWords:\n","        if word in tempArray:\n","          temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","        else :\n","          temp.append(0.0)\n","      tfData[key] = list(np.multiply(temp,tfData[key]))\n","\n","    for key in validstore:\n","      tempArray = validstore[key]\n","      temp = []\n","      for word in dictionaryWords:\n","        if word in tempArray and word in wordval:\n","          temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","        else :\n","          temp.append(0.0)\n","      tfDataTest[key] = list(np.multiply(temp,tfDataTest[key]))\n","        \n","    return tfData,tfDataTest\n","\n","  numberofDoc = len(datastore)\n","  #print(numberofDoc)\n","\n","  idfData = {}\n","  tfidfDataArray,tfidfDataTestArray = computeIDF(datastore,validstore,numberofDoc,uniqueWord,wordval,tfData,tfDataTest)\n","  \n","  def cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval):\n","    listVal1 = np.sqrt(np.sum(np.square(testingBinArrCS),axis=1))\n","    listVal2 = np.sqrt(np.sum(np.square(tfidfDataTestArray[keyval])))\n","    dotResult = np.dot(testingBinArrCS,tfidfDataTestArray[keyval])\n","    finalResult = np.divide(dotResult,np.dot(listVal1,listVal2))\n","    finalResult, headerCS = (list(t) for t in zip(*sorted(zip(finalResult, headerCS),reverse=True)))\n","    return headerCS\n","\n","  headerCS = list(tfidfDataArray.keys())\n","  testingBinArrCS = np.array(list(tfidfDataArray.values()))\n","\n","  allBinDisctance = {}\n","  for keyval in tfidfDataTestArray:\n","    predicted = cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval)\n","    allBinDisctance[keyval] = predicted\n","\n","  if type is 'val':\n","    n_neighbors = [1,3,5]\n","  elif type is 'test':\n","    n_neighbors = [5]\n","\n","  for neighbor in n_neighbors:\n","    print(\"Neighbor :\",neighbor)\n","    totalCount = 0\n","    correctCount = 0\n","    for keyval in tfidfDataTestArray:\n","      predArray = allBinDisctance[keyval]\n","      voting_count = {}\n","      for val in predArray[:neighbor]:\n","        if val[0] in voting_count:\n","          voting_count[val[0]] += 1\n","        else :\n","          voting_count[val[0]] = 1\n","\n","      prediction = max(voting_count, key=voting_count.get)\n","      if prediction == keyval[0]:\n","        correctCount += 1\n","      totalCount += 1\n","      \n","    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","  print()\n","  return correctCount/float(totalCount)\n","\n","\n","\n","train_path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"\n","#topic name reading\n","topic_number =0\n","topic_list = {}\n","with open(\"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/topics.txt\") as fp:\n","  lines = fp.readlines()\n","   \n","  for line in lines:\n","    line = line.strip()\n","    topic_list[line] = topic_number\n","    topic_number += 1\n","    '''\n","    if topic_number == 3 :\n","      break\n","    '''\n","fp.close()\n","print(topic_list)\n","topicLength = len(topic_list)\n","# train data and validation data \n","topicDataSize = 700\n","trainDataSize = 500\n","validationDataSize = 200\n","allResult = {}\n","trainDataStore = {}\n","validdataStore = {}\n","count = 0\n","\n","position_array = []\n","for i in range(0,topicDataSize):\n","  position_array.append(i)\n","\n","for topic in topic_list:\n","  \n","  with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","    content = file.read()\n","    soup = bs(content)\n","    rowList = soup.findAll(\"row\")\n","    count = 0\n","    for row in rowList[0:trainDataSize]:\n","      topicPair = (topic,position_array[count])\n","      if len(row.get('body')) == 0:\n","        continue;\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      for word in word_list:\n","        pairWord = (word,topic_list[topic])\n","        if pairWord in allResult:\n","          allResult[pairWord] += 1\n","        else:\n","          allResult[pairWord] = 1\n","      count += 1\n","      trainDataStore[topicPair] = np.array(word_list)\n","\n","    #validation data\n","    for i in range(trainDataSize,trainDataSize+validationDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      validdataStore[topicPair] = np.array(word_list)\n","\n","print(\"Document count :\",count)\n","\n","totalLoop = 50\n","acc = []\n","for loop in range(totalLoop):\n","  testSize = 10\n","  position_array = []\n","  for i in range(topicDataSize+loop*testSize,topicDataSize+loop*testSize+testSize):\n","    position_array.append(i)\n","  testDatastore = {}\n","\n","  for topic in topic_list:\n","    with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","      content = file.read()\n","      soup = bs(content)\n","      rowList = soup.findAll(\"row\")\n","      \n","      for i in range(len(position_array)):\n","        topicPair = (topic,position_array[i])\n","        row = rowList[position_array[i]]\n","        if len(row.get('body')) == 0:\n","          continue\n","        word_list = preprocessText(row.get('body'))\n","        if len(word_list) == 0:\n","          continue\n","        testDatastore[topicPair] = np.array(word_list) \n","\n","  accuracy = runTFIDF(trainDataStore,testDatastore,topic_list,topicLength,'test')\n","  acc.append(accuracy)\n","\n","print(acc)\n","print(len(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","{'Coffee': 0, 'Arduino': 1, 'Anime': 2, 'Astronomy': 3, 'Biology': 4, 'Chess': 5, 'Cooking': 6, 'Law': 7, 'Space': 8, 'Windows_Phone': 9, 'Wood_Working': 10}\n","Document count : 500\n","19550\n","Neighbor : 5\n","Total Correct Count:  97  Total Wrong Count:  11  Accuracy:  89.81481481481481\n","\n","19537\n","Neighbor : 5\n","Total Correct Count:  93  Total Wrong Count:  12  Accuracy:  88.57142857142857\n","\n","19551\n","Neighbor : 5\n","Total Correct Count:  87  Total Wrong Count:  17  Accuracy:  83.65384615384616\n","\n","19516\n","Neighbor : 5\n","Total Correct Count:  80  Total Wrong Count:  24  Accuracy:  76.92307692307692\n","\n","19687\n","Neighbor : 5\n","Total Correct Count:  85  Total Wrong Count:  20  Accuracy:  80.95238095238095\n","\n","19526\n","Neighbor : 5\n","Total Correct Count:  93  Total Wrong Count:  13  Accuracy:  87.73584905660377\n","\n","19507\n","Neighbor : 5\n","Total Correct Count:  90  Total Wrong Count:  18  Accuracy:  83.33333333333333\n","\n","19743\n","Neighbor : 5\n","Total Correct Count:  94  Total Wrong Count:  15  Accuracy:  86.23853211009174\n","\n","19499\n","Neighbor : 5\n","Total Correct Count:  91  Total Wrong Count:  18  Accuracy:  83.4862385321101\n","\n","19526\n","Neighbor : 5\n","Total Correct Count:  93  Total Wrong Count:  13  Accuracy:  87.73584905660377\n","\n","19599\n","Neighbor : 5\n","Total Correct Count:  87  Total Wrong Count:  20  Accuracy:  81.30841121495327\n","\n","19574\n","Neighbor : 5\n","Total Correct Count:  85  Total Wrong Count:  18  Accuracy:  82.52427184466019\n","\n","19563\n","Neighbor : 5\n","Total Correct Count:  94  Total Wrong Count:  15  Accuracy:  86.23853211009174\n","\n","19516\n","Neighbor : 5\n","Total Correct Count:  94  Total Wrong Count:  12  Accuracy:  88.67924528301887\n","\n","19552\n","Neighbor : 5\n","Total Correct Count:  88  Total Wrong Count:  22  Accuracy:  80.0\n","\n","19633\n","Neighbor : 5\n","Total Correct Count:  84  Total Wrong Count:  25  Accuracy:  77.06422018348624\n","\n"],"name":"stdout"}]}]}