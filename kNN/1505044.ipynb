{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1505044.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1igWM9raSFLS6OL8H6seEYdX1Seg3zuLS","authorship_tag":"ABX9TyOM7WIl4Hbum//S5AvHx3Ht"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"m0YwLXWkKchx","executionInfo":{"status":"ok","timestamp":1602938655025,"user_tz":-360,"elapsed":652282,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"2e50e8cb-13ba-4527-fde4-0c4dee918ea2","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip3 install bs4\n","!pip install spacy\n","\n","import nltk\n","import math\n","import os\n","import fnmatch\n","import random\n","import numpy as np\n","from xml.etree import ElementTree\n","import re\n","import string\n","import spacy\n","import scipy\n","from scipy import stats\n","from scipy.spatial import distance\n","from nltk.corpus import stopwords          \n","from nltk.stem import PorterStemmer        \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from bs4 import BeautifulSoup as bs\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","sp = spacy.load('en_core_web_sm')\n","spacy_stopwords = sp.Defaults.stop_words\n","\n","def preprocessText(text):\n","\n","  def makeTextLower(text):\n","    result = text.lower() \n","    return result \n","\n","  def MakeNumberRemove(text): \n","    #result = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', text)\n","    result = re.sub(r'[-+]?\\d+', '', text) \n","    return result \n","\n","  def MakeHTMLremove(text):\n","    cleancode = re.compile('<code>.*?</code>')\n","    cleanr = re.compile('<.*?>')\n","    cleanentity = re.compile('&.*;')\n","    cleantext = re.sub(cleancode, '', text)\n","    cleantext = re.sub(cleanr, ' ', cleantext)\n","    cleantext = re.sub(cleanentity, ' ', cleantext)  \n","    return cleantext\n","\n","  def makeRemoveExtraApace(text):\n","    result = \" \".join(text.split())  \n","    return result\n","\n","  def makeRemoveHyperLink(text):\n","    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","    return result  \n","\n","  def makeRemovePunctuation(text):\n","    result = text=text.translate((str.maketrans('','',string.punctuation)))\n","    return result\n","  \n","  def makeWordLemmatize(text):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = [lemmatizer.lemmatize(word,pos='v') for word in text] \n","    return lemmas\n","\n","  def removeStopWords(text):\n","    stopwords_english = stopwords.words('english')\n","    texts_clean = []\n","    #print(text)\n","    text = word_tokenize(text)\n","    for word in text: \n","      if word not in stopwords_english:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def removeSpacyStopWords(text,spacy_stopwords):\n","    texts_clean = []\n","    #print(text)\n","    for word in text: \n","      if word not in spacy_stopwords:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def makeStemming(text):\n","    ps = PorterStemmer() \n","    #words = word_tokenize(text)\n","    lemmas = [ps.stem(word) for word in text] \n","    return lemmas  \n","\n","  result = text\n","  result=re.sub(r\"<.+?>\",'',result)\n","  result=re.sub(r'[^\\x00-\\x7F]+','',result)\n","  result = MakeHTMLremove(result)\n","  result = makeTextLower(result)\n","  result = MakeNumberRemove(result)\n","  result = makeRemoveHyperLink(result)\n","  result = makeRemovePunctuation(result)\n","  result = removeStopWords(result)\n","  result = removeSpacyStopWords(result,spacy_stopwords)\n","  result = makeWordLemmatize(result)\n","  result = makeStemming(result)\n","  return result  \n","\n","def naiveBayes(allResult,trainDataSize,validationDataSize,topic_list,topicLength,smfactorList,type):\n","  trainDataSize = trainDataSize\n","  validationDataSize = validationDataSize\n","  def trainNaive(allResult,topicLength,smFactor):\n","  \n","    uniqueWord = []\n","    for word in allResult.keys():\n","      if word[0] not in uniqueWord:\n","        uniqueWord.append(word[0])\n","    uniqueWordSize = len(uniqueWord)\n","    #print(uniqueWord[:50])\n","\n","    topicWiseCount = {}\n","    for wordPair in allResult.keys():\n","      if wordPair in topicWiseCount.keys():\n","        topicWiseCount[wordPair[1]] = topicWiseCount[wordPair[1]] + allResult[wordPair]\n","      else :\n","        topicWiseCount[wordPair[1]] = 1\n","\n","    laplaceSmoothingCalc = {}\n","    for word in uniqueWord:\n","      for topic in range(topicLength):\n","        \n","        wordPair = (word,topic)\n","        prevCount = 0\n","        if wordPair in allResult:\n","          prevCount = allResult[wordPair]\n","        probability = (prevCount+smFactor)/(float(topicWiseCount[topic])+smFactor*uniqueWordSize)\n","\n","        laplaceSmoothingCalc[wordPair] = probability\n","    \n","    return topicWiseCount,laplaceSmoothingCalc,uniqueWordSize\n","\n","  def predictNaive(word_list,laplaceSmoothingCalc,topicLength,topicWiseCount,uniqueWordSize,smFactor):\n","  \n","    maxProb = 0.0\n","    currTopic = 0\n","    allDistances = []\n","    for topic in range(topicLength):\n","      multiProb = math.log(1/float(topicLength))\n","      for word in word_list:\n","        wordPair = (word,topic)\n","        if wordPair in laplaceSmoothingCalc.keys():\n","          multiProb = multiProb + math.log(laplaceSmoothingCalc[wordPair])\n","        else :\n","          multiProb = multiProb + math.log((smFactor)/(topicWiseCount[topic]+smFactor*uniqueWordSize))\n","          #continue\n","      #print(multiProb)\n","      allDistances.append((topic,word,multiProb))\n","    allDistances.sort(key=lambda x: x[2],reverse=True)\n","    currTopic = allDistances[0][0]\n","    #print()\n","    return currTopic\n","  #print(trainDataSize)\n","  if type is 'val':\n","    for smFactor in smfactorList:\n","      print(\"smoothing factor constant :\",smFactor)\n","      topicWiseCount,laplaceSmoothingCalc,uniqueWordSize = trainNaive(allResult,topicLength,smFactor)\n","      #print(laplaceSmoothingCalc[(\"get\",1)])\n","      #print(uniqueWordSize)\n","      correctCount = 0\n","      totalCount = 0\n","      for key in topic_list:\n","        path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"+key+\".xml\"\n","        trainDataSize = trainDataSize\n","        validationDataSize = validationDataSize\n","        #print(trainDataSize)\n","        with open(path,'r',encoding='utf-8') as file:\n","          content = file.read()\n","          soup = bs(content)\n","          rowList = soup.findAll(\"row\")\n","          #correctCount = 0\n","          #print(trainDataSize)\n","          for row in rowList[trainDataSize:trainDataSize+validationDataSize+1]:\n","            #print(row.get('body'))\n","            #print()\n","            if len(row.get('body')) == 0:\n","              continue\n","            word_list = preprocessText(row.get('body'))\n","            if len(word_list) == 0:\n","              continue\n","            if predictNaive(word_list,laplaceSmoothingCalc,topicLength,topicWiseCount,uniqueWordSize,smFactor) is topic_list[key]:\n","              correctCount += 1\n","            totalCount += 1\n","      print((correctCount/float(totalCount))*100)\n","      print()\n","    return correctCount/float(totalCount)\n","\n","  elif type is 'test':\n","    for smFactor in smfactorList:\n","      print(\"smoothing factor constant :\",smFactor)\n","      topicWiseCount,laplaceSmoothingCalc,uniqueWordSize = trainNaive(allResult,topicLength,smFactor)\n","      #print(laplaceSmoothingCalc[(\"get\",1)])\n","      #print(uniqueWordSize)\n","      totalLoop = 3\n","      testSize = 10\n","      acc = []\n","      for loop in range(totalLoop):\n","        correctCount = 0\n","        totalCount = 0\n","        for key in topic_list:\n","          path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"+key+\".xml\"\n","          trainDataSize = trainDataSize\n","          #validationDataSize = validationDataSize\n","          #print(trainDataSize)\n","          with open(path,'r',encoding='utf-8') as file:\n","            content = file.read()\n","            soup = bs(content)\n","            rowList = soup.findAll(\"row\")\n","            #correctCount = 0\n","            #print(trainDataSize)\n","            for row in rowList[trainDataSize+validationDataSize+loop*testSize:trainDataSize++validationDataSize+loop*testSize+testSize]:\n","              #print(row.get('body'))\n","              #print()\n","              if len(row.get('body')) == 0:\n","                continue\n","              word_list = preprocessText(row.get('body'))\n","              if len(word_list) == 0:\n","                continue\n","              if predictNaive(word_list,laplaceSmoothingCalc,topicLength,topicWiseCount,uniqueWordSize,smFactor) is topic_list[key]:\n","                correctCount += 1\n","              totalCount += 1\n","\n","        acc.append(correctCount/float(totalCount))\n","    return acc \n","\n","def runKNN(trainDataStore,validdataStore,topic_list,topicLength):\n","\n","  def BinaryRepresent(trainDataStore,position_array,topic_list,testStore):\n","    #find unique word\n","    uniqueWord = []\n","    for pair in trainDataStore:\n","      wordList = trainDataStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    \n","    for pair in testStore:\n","      wordList = testStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    #print(uniqueWord)\n","\n","    # binary representation creation\n","    binary_represent_final_train = {}\n","    binary_represent_final_test = {}\n","    bow_represent_final_train = {}\n","    bow_represent_final_test = {}\n","    \n","    for pair in trainDataStore:\n","      wordList = list(trainDataStore[pair])\n","      tempStore = []\n","      tempStoreBow = []\n","      for word in uniqueWord:\n","        if word in wordList:\n","          tempStore.append(1)\n","        else:\n","          tempStore.append(0)\n","        tempStoreBow.append(wordList.count(word))\n","      binary_represent_final_train[pair] = tempStore\n","      bow_represent_final_train[pair] = tempStoreBow\n","\n","    for pair in testStore:\n","      wordList = list(testStore[pair])\n","      tempStore = []\n","      tempStoreBow = []\n","      for word in uniqueWord:\n","        if word in wordList:\n","          tempStore.append(1)\n","        else:\n","          tempStore.append(0)\n","        tempStoreBow.append(wordList.count(word))\n","      binary_represent_final_test[pair] = tempStore  \n","      bow_represent_final_test[pair] = tempStoreBow\n","    \n","    return binary_represent_final_train,uniqueWord,binary_represent_final_test,bow_represent_final_train,bow_represent_final_test\n","\n","  def hammingDistance(trainArray,testArray):\n","    dist = list(distance.cdist(trainArray,[testArray],'cityblock').flatten())\n","    return dist\n","\n","  def predictionBinary(header,testingBinArr,binary_represent_final_test,dataName):\n","    dist = list(hammingDistance(testingBinArr,binary_represent_final_test[dataName]))\n","    dist, header = (list(t) for t in zip(*sorted(zip(dist, header))))\n","    return header\n","\n","  def euclideanDistance(trainArray,testArray):\n","    distance = np.sqrt(np.sum(np.square(np.subtract(trainArray,testArray)),axis=1))\n","    return distance \n","\n","  def predictionBow(headerBow,testingBinArrBow,bow_represent_final_test,dataName):\n","    distance = list(euclideanDistance(testingBinArrBow,bow_represent_final_test[dataName]))\n","    distance, headerBow = (list(t) for t in zip(*sorted(zip(distance, headerBow))))\n","    return headerBow     \n","\n","  def performanceEvaluationKNN(trainDataStore,validdataStore,position_array,topic_list):\n","    \n","    binary_represent_final_train,uniqueWord,binary_represent_final_test,bow_represent_final_train,bow_represent_final_test = BinaryRepresent(trainDataStore,position_array,topic_list,validdataStore)\n","    #print(bow_represent_final_train)\n","    print(\"bow represent created\")\n","    ####################\n","    header = list(binary_represent_final_train.keys())\n","    testingBinArr = np.array(list(binary_represent_final_train.values()))\n","  \n","    headerBow = list(bow_represent_final_train.keys())\n","    testingBinArrBow = np.array(list(bow_represent_final_train.values()))\n","      \n","    # binary representation\n","    print(\"bin training started\")\n","    allBinDisctance = {}\n","    for dataName in binary_represent_final_test:\n","        predicted = predictionBinary(header,testingBinArr,binary_represent_final_test,dataName)\n","        allBinDisctance[dataName] = predicted\n","\n","    n_neighbors = [1,3,5]\n","    for neighbor in n_neighbors:\n","      print(\"Neighbor :\",neighbor)\n","      totalCount = 0\n","      correctCount = 0\n","      for dataName in binary_represent_final_test:\n","        predArray = allBinDisctance[dataName]\n","        voting_count = {}\n","        for val in predArray[:neighbor]:\n","          if val[0] in voting_count:\n","            voting_count[val[0]] += 1\n","          else :\n","            voting_count[val[0]] = 1\n","\n","        prediction = max(voting_count, key=voting_count.get)\n","        if prediction == dataName[0]:\n","          correctCount += 1\n","        totalCount += 1\n","        \n","      print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","    print()\n","    \n","    # bag of words\n","    print(\"bow training started\")\n","    allBinDisctance = {}\n","    for dataName in bow_represent_final_test:\n","        predicted = predictionBow(headerBow,testingBinArrBow,bow_represent_final_test,dataName)\n","        allBinDisctance[dataName] = predicted\n","\n","    n_neighbors = [1,3,5]\n","    for neighbor in n_neighbors:\n","      print(\"Neighbor :\",neighbor)\n","      totalCount = 0\n","      correctCount = 0\n","      for dataName in bow_represent_final_test:\n","        predArray = allBinDisctance[dataName]\n","        voting_count = {}\n","        for val in predArray[:neighbor]:\n","          if val[0] in voting_count:\n","            voting_count[val[0]] += 1\n","          else :\n","            voting_count[val[0]] = 1\n","\n","        prediction = max(voting_count, key=voting_count.get)\n","        if prediction == dataName[0]:\n","          correctCount += 1\n","        totalCount += 1\n","        \n","      print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","    print()\n","    \n","    \n","  performanceEvaluationKNN(trainDataStore,validdataStore,position_array,topic_list)\n","\n","\n","\n","def runTFIDF(datastore,validstore,topic_list,topicLength,type):\n","\n","  def numberDocsForWord(datastore,words):\n","    wordVal = {}\n","    value = list(datastore.values())\n","    for word in words:\n","      count = sum(word in e for e in value)    \n","      wordVal[word] = count\n","    return wordVal\n","\n","  def uniqueWordFunc(trainDataStore,testStore):\n","    uniqueWord = []\n","    for pair in trainDataStore:\n","      wordList = trainDataStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    wordval = numberDocsForWord(datastore,uniqueWord)\n","    for pair in testStore:\n","      wordList = testStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    return uniqueWord,wordval\n","\n","  def countData(datastore,validstore,uniqueWord):\n","    countDatastore = {}\n","    countDatastoreTest = {}\n","    \n","    for key in datastore:\n","      tempArray = list(datastore[key])\n","      wordDictA = []\n","      for word in uniqueWord:\n","        wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","      countDatastore[key] = wordDictA\n","      \n","    for key in validstore:\n","      tempArray = list(validstore[key])\n","      wordDictA = []\n","      for word in uniqueWord:\n","        wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","      countDatastoreTest[key] = wordDictA\n","    \n","    return countDatastore,countDatastoreTest\n","\n","  uniqueWord,wordval = uniqueWordFunc(datastore,validstore)    \n","  tfData,tfDataTest = countData(datastore,validstore,uniqueWord)\n","  print(len(uniqueWord))\n","\n","  def computeIDF(datastore,validstore,numberofDoc,dictionaryWords,wordval,tfData,tfDataTest):\n","    \n","    idfDict = {}\n","    idfDict = dict.fromkeys(set(dictionaryWords), 0)\n","    \n","    for key in datastore:\n","      tempArray = datastore[key]\n","      temp = []\n","      for word in dictionaryWords:\n","        if word in tempArray:\n","          temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","        else :\n","          temp.append(0.0)\n","      tfData[key] = list(np.multiply(temp,tfData[key]))\n","\n","    for key in validstore:\n","      tempArray = validstore[key]\n","      temp = []\n","      for word in dictionaryWords:\n","        if word in tempArray and word in wordval:\n","          temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","        else :\n","          temp.append(0.0)\n","      tfDataTest[key] = list(np.multiply(temp,tfDataTest[key]))\n","        \n","    return tfData,tfDataTest\n","\n","  numberofDoc = len(datastore)\n","  #print(numberofDoc)\n","\n","  idfData = {}\n","  tfidfDataArray,tfidfDataTestArray = computeIDF(datastore,validstore,numberofDoc,uniqueWord,wordval,tfData,tfDataTest)\n","  \n","  def cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval):\n","    listVal1 = np.sqrt(np.sum(np.square(testingBinArrCS),axis=1))\n","    listVal2 = np.sqrt(np.sum(np.square(tfidfDataTestArray[keyval])))\n","    dotResult = np.dot(testingBinArrCS,tfidfDataTestArray[keyval])\n","    finalResult = np.divide(dotResult,np.dot(listVal1,listVal2))\n","    finalResult, headerCS = (list(t) for t in zip(*sorted(zip(finalResult, headerCS),reverse=True)))\n","    return headerCS\n","\n","  headerCS = list(tfidfDataArray.keys())\n","  testingBinArrCS = np.array(list(tfidfDataArray.values()))\n","\n","  allBinDisctance = {}\n","  for keyval in tfidfDataTestArray:\n","    predicted = cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval)\n","    allBinDisctance[keyval] = predicted\n","\n","  if type is 'val':\n","    n_neighbors = [1,3,5]\n","  elif type is 'test':\n","    n_neighbors = [5]\n","\n","  for neighbor in n_neighbors:\n","    print(\"Neighbor :\",neighbor)\n","    totalCount = 0\n","    correctCount = 0\n","    for keyval in tfidfDataTestArray:\n","      predArray = allBinDisctance[keyval]\n","      voting_count = {}\n","      for val in predArray[:neighbor]:\n","        if val[0] in voting_count:\n","          voting_count[val[0]] += 1\n","        else :\n","          voting_count[val[0]] = 1\n","\n","      prediction = max(voting_count, key=voting_count.get)\n","      if prediction == keyval[0]:\n","        correctCount += 1\n","      totalCount += 1\n","      \n","    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","  print()\n","  return correctCount/float(totalCount)\n","\n","train_path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"\n","#topic name reading\n","topic_number =0\n","topic_list = {}\n","with open(\"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/topics.txt\") as fp:\n","  lines = fp.readlines()\n","   \n","  for line in lines:\n","    line = line.strip()\n","    topic_list[line] = topic_number\n","    topic_number += 1\n","    \n","    if topic_number == 3 :\n","      break\n","    \n","fp.close()\n","print(topic_list)\n","topicLength = len(topic_list)\n","# train data and validation data \n","topicDataSize = 700\n","trainDataSize = 500\n","validationDataSize = 200\n","allResult = {}\n","trainDataStore = {}\n","validdataStore = {}\n","count = 0\n","\n","position_array = []\n","for i in range(0,topicDataSize):\n","  position_array.append(i)\n","\n","for topic in topic_list:\n","  \n","  with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","    content = file.read()\n","    soup = bs(content)\n","    rowList = soup.findAll(\"row\")\n","    count = 0\n","    for row in rowList[0:trainDataSize]:\n","      topicPair = (topic,position_array[count])\n","      if len(row.get('body')) == 0:\n","        continue;\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      for word in word_list:\n","        pairWord = (word,topic_list[topic])\n","        if pairWord in allResult:\n","          allResult[pairWord] += 1\n","        else:\n","          allResult[pairWord] = 1\n","      count += 1\n","      trainDataStore[topicPair] = np.array(word_list)\n","\n","    #validation data\n","    for i in range(trainDataSize,trainDataSize+validationDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      validdataStore[topicPair] = np.array(word_list)\n","\n","print(\"Document count :\",count)\n","\n","print(\"Naive bayes running...\")\n","smfactorList = [.001,.005,.01,.05,.08,0.1,0.3,0.5,0.8,1,10,20]\n","accuracy = naiveBayes(allResult,trainDataSize,validationDataSize,topic_list,topicLength,smfactorList,'val')\n","print(\"KNN running..\")\n","runKNN(trainDataStore,validdataStore,topic_list,topicLength)\n","print(\"TF-IDF running...\")\n","runTFIDF(trainDataStore,validdataStore,topic_list,topicLength,'val')\n","\n","#######################################\n","# Test Naive Bayes\n","smfactorList = [.05]\n","accuracyList = naiveBayes(allResult,trainDataSize,validationDataSize,topic_list,topicLength,smfactorList,'test')\n","print(accuracyList)\n","print(len(accuracyList))\n","\n","###########################################\n","# Test TFIDF\n","totalLoop = 3\n","acc = []\n","for loop in range(totalLoop):\n","  testSize = 10\n","  position_array = []\n","  for i in range(topicDataSize+loop*testSize,topicDataSize+loop*testSize+testSize):\n","    position_array.append(i)\n","  testDatastore = {}\n","\n","  for topic in topic_list:\n","    with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","      content = file.read()\n","      soup = bs(content)\n","      rowList = soup.findAll(\"row\")\n","      \n","      for i in range(len(position_array)):\n","        topicPair = (topic,position_array[i])\n","        row = rowList[position_array[i]]\n","        if len(row.get('body')) == 0:\n","          continue\n","        word_list = preprocessText(row.get('body'))\n","        if len(word_list) == 0:\n","          continue\n","        testDatastore[topicPair] = np.array(word_list) \n","\n","  accuracy = runTFIDF(trainDataStore,testDatastore,topic_list,topicLength,'test')\n","  acc.append(accuracy)\n","\n","print(acc)\n","print(len(acc))\n","\n","tStat,p = scipy.stats.ttest_rel(accuracyList,acc)\n","print(\"t-stat value:\",tStat)\n","print(\"p value:\",p)\n","\n","sig_level = [0.005,.01,.05]\n","\n","for sig in sig_level:\n","  print(\"Significance level :\",sig)\n","  if p>sig:\n","    print(\"Null hypothesis occurs\")\n","    print(\"Similar mean and similar method\")\n","    print()\n","  else:\n","    print(\"Means are not similar and different method\")\n","    print()\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","{'Coffee': 0, 'Arduino': 1, 'Anime': 2}\n","Document count : 485\n","Naive bayes running...\n","smoothing factor constant : 0.001\n","98.11643835616438\n","\n","smoothing factor constant : 0.005\n","98.11643835616438\n","\n","smoothing factor constant : 0.01\n","98.28767123287672\n","\n","smoothing factor constant : 0.05\n","98.11643835616438\n","\n","smoothing factor constant : 0.08\n","98.11643835616438\n","\n","smoothing factor constant : 0.1\n","98.11643835616438\n","\n","smoothing factor constant : 0.3\n","97.77397260273972\n","\n","smoothing factor constant : 0.5\n","97.77397260273972\n","\n","smoothing factor constant : 0.8\n","97.94520547945206\n","\n","smoothing factor constant : 1\n","97.94520547945206\n","\n","smoothing factor constant : 10\n","96.57534246575342\n","\n","smoothing factor constant : 20\n","96.57534246575342\n","\n","KNN running..\n","bow represent created\n","bin training started\n","Neighbor : 1\n","Total Correct Count:  446  Total Wrong Count:  135  Accuracy:  76.76419965576592\n","Neighbor : 3\n","Total Correct Count:  433  Total Wrong Count:  148  Accuracy:  74.52667814113597\n","Neighbor : 5\n","Total Correct Count:  457  Total Wrong Count:  124  Accuracy:  78.65748709122204\n","\n","bow training started\n","Neighbor : 1\n","Total Correct Count:  466  Total Wrong Count:  115  Accuracy:  80.20654044750431\n","Neighbor : 3\n","Total Correct Count:  454  Total Wrong Count:  127  Accuracy:  78.14113597246127\n","Neighbor : 5\n","Total Correct Count:  467  Total Wrong Count:  114  Accuracy:  80.37865748709122\n","\n","TF-IDF running...\n","11203\n","Neighbor : 1\n","Total Correct Count:  552  Total Wrong Count:  29  Accuracy:  95.00860585197934\n","Neighbor : 3\n","Total Correct Count:  558  Total Wrong Count:  23  Accuracy:  96.04130808950086\n","Neighbor : 5\n","Total Correct Count:  567  Total Wrong Count:  14  Accuracy:  97.59036144578313\n","\n","smoothing factor constant : 0.05\n","[1.0, 0.9666666666666667, 1.0]\n","3\n","9223\n","Neighbor : 5\n","Total Correct Count:  29  Total Wrong Count:  1  Accuracy:  96.66666666666667\n","\n","9298\n","Neighbor : 5\n","Total Correct Count:  29  Total Wrong Count:  1  Accuracy:  96.66666666666667\n","\n","9245\n","Neighbor : 5\n","Total Correct Count:  30  Total Wrong Count:  0  Accuracy:  100.0\n","\n","[0.9666666666666667, 0.9666666666666667, 1.0]\n","3\n","t-stat value: 0.9999999999999999\n","p value: 0.42264973081037427\n","Significance level : 0.005\n","Null hypothesis occurs\n","Similar mean and similar method\n","\n","Significance level : 0.01\n","Null hypothesis occurs\n","Similar mean and similar method\n","\n","Significance level : 0.05\n","Null hypothesis occurs\n","Similar mean and similar method\n","\n"],"name":"stdout"}]}]}