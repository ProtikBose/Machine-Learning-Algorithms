{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1505044_kNN.ipynb","provenance":[],"mount_file_id":"1g9ybzdfWT1713CrvD231MX2KAzwStiGG","authorship_tag":"ABX9TyOhNIgUdzZDMwjDk56eG+NI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"B0r1TpPdUtuP","executionInfo":{"status":"ok","timestamp":1602600653782,"user_tz":-360,"elapsed":5944,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"22006278-62fd-4220-88cf-c7a55d2c5c7a","colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["!pip3 install bs4\n","!pip install spacy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oife2IfwlFgX","executionInfo":{"status":"ok","timestamp":1602600653783,"user_tz":-360,"elapsed":5925,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"e638813a-6d3d-4c90-ebb4-b1f22a7015b1","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["list1 = [\"a\",\"b\",\"b\",\"d\"]\n","list1.count(\"b\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"chiWiRQpOwfS"},"source":["import nltk\n","import math\n","import os\n","import fnmatch\n","import random\n","import numpy as np\n","from xml.etree import ElementTree\n","import re\n","import string\n","import spacy\n","from nltk.corpus import stopwords          \n","from nltk.stem import PorterStemmer        \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from scipy.spatial import distance\n","from bs4 import BeautifulSoup as bs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGCT-UiL-Bco","executionInfo":{"status":"ok","timestamp":1602600653784,"user_tz":-360,"elapsed":5894,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"fb59d12f-a975-4a1c-ce4d-1943f3b41571","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# download the stopwords from NLTK\n","#nltk.download('all')\n","# download the stopwords from NLTK\n","#nltk.download('all')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"code","metadata":{"id":"k9bmr9u3EhjO"},"source":["def makeTextLower(text):\n","  result = text.lower() \n","  return result "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gccnY93EEowm"},"source":["def MakeNumberRemove(text): \n","  #result = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', text)\n","  result = re.sub(r'[-+]?\\d+', '', text) \n","  return result "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXvVsv0cEr5_"},"source":["def MakeHTMLremove(text):\n","  '''\n","  result = re.compile('(<.*?>)')\n","  result = result.sub(r'', text) \n","  '''\n","  cleancode = re.compile('<code>.*?</code>')\n","  cleanr = re.compile('<.*?>')\n","  cleanentity = re.compile('&.*;')\n","  cleantext = re.sub(cleancode, '', text)\n","  cleantext = re.sub(cleanr, ' ', cleantext)\n","  cleantext = re.sub(cleanentity, ' ', cleantext)\n","  \n","  return cleantext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ELJRi0BOEuNw"},"source":["def makeRemoveExtraApace(text):\n","  result = \" \".join(text.split())  \n","  return result   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZB6gdVDExDy"},"source":["def makeRemoveHyperLink(text):\n","  result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0h_ThUBEzk1"},"source":["def makeRemovePunctuation(text):\n","  \n","  #print(string.punctuation)\n","  #translator = str.maketrans('', '', string.punctuation)\n","  result = text=text.translate((str.maketrans('','',string.punctuation)))\n","  return result\n","  \n","  '''\n","  puncts = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","\n","  for punct in puncts:\n","    if punct in text:\n","      text = text.replace(punct, '')\n","\n","  return text\n","  '''\n","#makeRemovePunctuation(\"Hi;\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIagPNV1E4I1"},"source":["def makeWordLemmatize(text):\n","  lemmatizer = WordNetLemmatizer()\n","  #word_tokens = word_tokenize(text)\n","  lemmas = [lemmatizer.lemmatize(word,pos='v') for word in text] \n","  return lemmas "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yetF8QwBSpsf"},"source":["def cleanTextData(text):\n","\n","  #uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n","  text = text.lower()\n","  # specific\n","  text = re.sub(r\"won't\", \"will not\", text)\n","  text = re.sub(r\"can\\'t\", \"can not\", text)\n","\n","  # general\n","  text = re.sub(r\"n\\'t\", \" not\", text)\n","  text = re.sub(r\"\\'re\", \" are\", text)\n","  text = re.sub(r\"\\'s\", \" is\", text)\n","  text = re.sub(r\"\\'d\", \" would\", text)\n","  text = re.sub(r\"\\'ll\", \" will\", text)\n","  text = re.sub(r\"\\'t\", \" not\", text)\n","  text = re.sub(r\"\\'ve\", \" have\", text)\n","  text = re.sub(r\"\\'m\", \" am\", text)\n","  text = re.sub(r\"\\n\", \"\", text)\n","  #text = re.sub(uri_re, \"\", text)\n","  text = re.sub(r\"what's\", \"what is \", text)\n","  text = re.sub(r\"\\'s\", \" \", text)\n","  text = re.sub(r\"\\'ve\", \" have \", text)\n","  text = re.sub(r\"can't\", \"can not \", text)\n","  text = re.sub(r\"n't\", \" not \", text)\n","  text = re.sub(r\"i'm\", \"i am \", text)\n","  text = re.sub(r\"\\'re\", \" are \", text)\n","  text = re.sub(r\"\\'d\", \" would \", text)\n","  text = re.sub(r\"\\'ll\", \" will \", text)\n","  text = re.sub(r\"\\'scuse\", \" excuse \", text)\n","  text = re.sub(r\"\\'\\n\", \" \", text)\n","  text = re.sub(r\"\\'\\xa0\", \" \", text)\n","  text = re.sub('\\s+', ' ', text)\n","  text = text.strip(' ')\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MM6hQ8WE7n6"},"source":["def removeStopWords(text):\n","  stopwords_english = stopwords.words('english')\n","  texts_clean = []\n","  #print(text)\n","  text = word_tokenize(text)\n","  for word in text: \n","    if word not in stopwords_english:  \n","        texts_clean.append(word)\n","  return texts_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzPbn_m_aIOV"},"source":["sp = spacy.load('en_core_web_sm')\n","spacy_stopwords = sp.Defaults.stop_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_wt1Jn5YiJn"},"source":["def removeSpacyStopWords(text,spacy_stopwords):\n","  \n","  texts_clean = []\n","  #print(text)\n","  for word in text: \n","    if word not in spacy_stopwords:  \n","        texts_clean.append(word)\n","  return texts_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAX1KGfwdF6z"},"source":["def makeStemming(text):\n","  ps = PorterStemmer() \n","  #words = word_tokenize(text)\n","  lemmas = [ps.stem(word) for word in text] \n","  return lemmas  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wCzRBOSE-s5"},"source":["def preprocessText(text):\n","\n","  '''\n","  - html remove\n","  - Transforming abbreviations\n","  - Removing punctuation\n","  - Lemmatizing/stemming words ?\n","  - Removing stop words\n","  '''\n","  #result=re.sub(r\"<a.*</a>\", '', text)\n","  result = text\n","  result=re.sub(r\"<.+?>\",'',result)\n","  #result = makeTextLower(text)\n","  result=re.sub(r'[^\\x00-\\x7F]+','',result)\n","  result = MakeHTMLremove(result)\n","  #result = makeRemoveExtraApace(result)\n","  result = makeTextLower(result)\n","  \n","  #result = cleanTextData(result)\n","  result = MakeNumberRemove(result)\n","  result = makeRemoveHyperLink(result)\n","  result = makeRemovePunctuation(result)\n","  \n","  result = removeStopWords(result)\n","  result = removeSpacyStopWords(result,spacy_stopwords)\n","  result = makeWordLemmatize(result)\n","  result = makeStemming(result)\n","  \n","  \n","  #result = makeTextLower(result)\n","\n","  return result  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2XGmFohPIJ_","executionInfo":{"status":"ok","timestamp":1602600716347,"user_tz":-360,"elapsed":68324,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"3c8522e0-e024-4df1-b364-ede67f4c6c09","colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["train_path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"\n","#topic name reading\n","topic_number =0\n","topic_list = {}\n","with open(\"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/topics.txt\") as fp:\n","  lines = fp.readlines() \n","  for line in lines:\n","    line = line.strip()\n","    topic_list[line] = topic_number\n","    topic_number += 1\n","    '''\n","    if topic_number == 3 :\n","      break\n","    '''\n","fp.close()\n","print(topic_list)\n","topicLength = len(topic_list)\n","#dom = ElementTree.parse(train_path+xml_file[0])\n","#topic_list = {\"Anime\":0,\"Arduino\":1,\"Biology\":2,\"Chess\":3}\n","#topicLength = len(topic_list)\n","#print(topicLength)\n","#each topic data size\n","\n","# train data and validation data \n","topicDataSize = 700\n","trainDataSize = 500\n","validationDataSize = 200\n"," \n","position_array = []\n","for i in range(0,topicDataSize):\n","  position_array.append(i)\n","#print(position_array)\n","#random.seed(20)\n","#random.shuffle(position_array)\n","#print(position_array)\n","\n","trainDataStore = {}\n","validdataStore = {}\n","\n","#data collection\n","for topic in topic_list:\n","  with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","    content = file.read()\n","    soup = bs(content)\n","    rowList = soup.findAll(\"row\")\n","    \n","    #train data\n","    for i in range(0,trainDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      trainDataStore[topicPair] = np.array(word_list)\n","    \n","    #validation data\n","    for i in range(trainDataSize,trainDataSize+validationDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      validdataStore[topicPair] = np.array(word_list)\n","'''\n","for i in validdataStore:\n","  print(i)\n","  print(validdataStore[i])\n","'''      "],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'Coffee': 0, 'Arduino': 1, 'Anime': 2, 'Astronomy': 3, 'Biology': 4, 'Chess': 5, 'Cooking': 6, 'Law': 7, 'Space': 8, 'Windows_Phone': 9, 'Wood_Working': 10}\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfor i in validdataStore:\\n  print(i)\\n  print(validdataStore[i])\\n'"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"code","metadata":{"id":"KISH8mqJuBI3"},"source":["def BinaryRepresent(trainDataStore,position_array,topic_list,testStore):\n","  #find unique word\n","  uniqueWord = []\n","  for pair in trainDataStore:\n","    wordList = trainDataStore[pair]\n","    for word in wordList:\n","      if word not in uniqueWord:\n","        uniqueWord.append(word)\n","  \n","  for pair in testStore:\n","    wordList = testStore[pair]\n","    for word in wordList:\n","      if word not in uniqueWord:\n","        uniqueWord.append(word)\n","  #print(uniqueWord)\n","\n","  # binary representation creation\n","  binary_represent_final_train = {}\n","  binary_represent_final_test = {}\n","  bow_represent_final_train = {}\n","  bow_represent_final_test = {}\n","  \n","  for pair in trainDataStore:\n","    wordList = list(trainDataStore[pair])\n","    tempStore = []\n","    tempStoreBow = []\n","    for word in uniqueWord:\n","      if word in wordList:\n","        tempStore.append(1)\n","      else:\n","        tempStore.append(0)\n","      tempStoreBow.append(wordList.count(word))\n","    binary_represent_final_train[pair] = tempStore\n","    bow_represent_final_train[pair] = tempStoreBow\n","\n","  for pair in testStore:\n","    wordList = list(testStore[pair])\n","    tempStore = []\n","    tempStoreBow = []\n","    for word in uniqueWord:\n","      if word in wordList:\n","        tempStore.append(1)\n","      else:\n","        tempStore.append(0)\n","      tempStoreBow.append(wordList.count(word))\n","    binary_represent_final_test[pair] = tempStore  \n","    bow_represent_final_test[pair] = tempStoreBow\n","  \n","  return binary_represent_final_train,uniqueWord,binary_represent_final_test,bow_represent_final_train,bow_represent_final_test\n","\n","#BinaryRepresent(trainDataStore,position_array,topic_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-HDft895Tes"},"source":["def hammingDistance(trainArray,testArray):\n","  \n","  dist = list(distance.cdist(trainArray,[testArray],'cityblock').flatten())\n","  return dist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYZSieke3h-3"},"source":["def predictionBinary(header,testingBinArr,binary_represent_final_test,dataName):\n","  \n","  dist = list(hammingDistance(testingBinArr,binary_represent_final_test[dataName]))\n","  dist, header = (list(t) for t in zip(*sorted(zip(dist, header))))\n","\n","  return header "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOXg0Mdeigli"},"source":["def euclideanDistance(trainArray,testArray):\n","  distance = np.sqrt(np.sum(np.square(np.subtract(trainArray,testArray)),axis=1))\n","  return distance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jZ5JT0fEidlB"},"source":["def predictionBow(headerBow,testingBinArrBow,bow_represent_final_test,dataName):\n","  \n","  \n","  #print(binary_represent_final_test)\n","  \n","  distance = list(euclideanDistance(testingBinArrBow,bow_represent_final_test[dataName]))\n","  distance, headerBow = (list(t) for t in zip(*sorted(zip(distance, headerBow))))\n","  \n","  return headerBow "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQIqWexUzeRM"},"source":["def calculateTF(wordDict, doc):\n","  tfDict = {}\n","  corpusCount = len(doc)\n","  for word, count in wordDict.items():\n","      tfDict[word] = count/float(corpusCount)\n","  return(tfDict)\n","\n","def callCalculateTF(trainDataStore,topic_list,validdataStore,uniqueWord):\n","  \n","  countDatastore = {}\n","  countDatastoreTest = {}\n","  for key in trainDataStore:\n","    tempArray = trainDataStore[key]\n","    wordDictA = dict.fromkeys(set(uniqueWord), 0)\n","    for word in tempArray:\n","      wordDictA[word]+=1\n","    countDatastore[key] = wordDictA\n","\n","  for key in validdataStore:\n","    tempArray = validdataStore[key]\n","    wordDictA = dict.fromkeys(set(uniqueWord), 0)\n","    for word in tempArray:\n","      wordDictA[word]+=1\n","    countDatastoreTest[key] = wordDictA\n","\n","\n","  tfData = {}\n","  tfDataTest = {}\n","  for key in countDatastore:\n","    val = calculateTF(countDatastore[key], trainDataStore[key])\n","    tfData[key] = val\n","  for key in countDatastoreTest:\n","    val = calculateTF(countDatastoreTest[key], validdataStore[key])\n","    tfDataTest[key] = val\n","\n","  return countDatastore,countDatastoreTest,tfData,tfDataTest\n","\n","def numberDocsForWord(datastore,words):\n","  wordVal = {}\n","  for word in words:\n","    count = 0\n","    for keys in datastore:\n","      if word in datastore[keys]:\n","        count += 1\n","    wordVal[word] = count\n","  return wordVal\n","\n","def calculateIDF(words,numberofDoc,datastore,dictionaryWords,wordval):\n","  \n","  idfDict = {}\n","  idfDict = dict.fromkeys(set(dictionaryWords), 0)\n","  #wordval = numberDocsForWord(datastore,dictionaryWords)\n","  #print(wordval)\n","  for word in words:\n","    idfDict[word] = math.log10(numberofDoc / (float(wordval[word]) + 1))\n","      \n","  return idfDict\n","\n","def callCalculateIDF(trainDataStore,validdataStore,countDatastore,countDatastoreTest):\n","  numberofDoc = len(trainDataStore)\n","  wordval = numberDocsForWord(trainDataStore,countDatastore[key].keys())\n","  idfData = {}\n","  for key in trainDataStore:\n","    idf = calculateIDF(trainDataStore[key],numberofDoc,trainDataStore,countDatastore[key].keys(),wordval)\n","    idfData[key] = idf\n","\n","  idfDataTest = {}\n","  for key in validdataStore:\n","    idf = calculateIDF(validdataStore[key],numberofDoc,trainDataStore,countDatastoreTest[key].keys())\n","    idfDataTest[key] = idf\n","\n","  return idfData,idfDataTest\n","\n","def calculateTFIDF(tfBow, idfs):\n","  tfidf = {}\n","  for word, val in tfBow.items():\n","      tfidf[word] = val*idfs[word]\n","  return tfidf\n","\n","def callCalculateTFIDF(tfData,tfDataTest,idfData,idfDataTest):\n","  tfidfData = {}\n","  tfidfDataTest = {}\n","\n","  for key in idfData:\n","    tfidfData[key] = calculateTFIDF(tfData[key],idfData[key])\n","  for key in idfDataTest:\n","    tfidfDataTest[key] = calculateTFIDF(tfDataTest[key],idfDataTest[key])\n","\n","  return tfidfData,tfidfDataTest\n","\n","def tfidfToArray(tfidfData,tfidfDataTest):\n","  tfidfDataArray = {}\n","  tfidfDataTestArray = {}\n","\n","  for key in tfidfData:\n","    tempArray = []\n","    for val in tfidfData[key]:\n","      tempArray.append(tfidfData[key][val])\n","    tfidfDataArray[key] = tempArray\n","  #print(tfidfDataArray)\n","\n","  for key in tfidfDataTest:\n","    tempArray = []\n","    for val in tfidfDataTest[key]:\n","      tempArray.append(tfidfDataTest[key][val])\n","    tfidfDataTestArray[key] = tempArray\n","  #print(tfidfDataArray)\n","  return tfidfDataArray,tfidfDataTestArray\n","\n","\n","def calculateTFIDFProcess(trainDataStore,topic_list,validdataStore,uniqueWord):\n","  countDatastore,countDatastoreTest,tfData,tfDataTest = callCalculateTF(trainDataStore,topic_list,validdataStore,uniqueWord)\n","  idfData,idfDataTest = callCalculateIDF(trainDataStore,validdataStore,countDatastore,countDatastoreTest)\n","  tfidfData,tfidfDataTest = callCalculateTFIDF(tfData,tfDataTest,idfData,idfDataTest)\n","  tfidfDataArray,tfidfDataTestArray = tfidfToArray(tfidfData,tfidfDataTest)\n","  return tfidfDataArray,tfidfDataTestArray\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVUdHvl3_GmR"},"source":["def calculateCosineSimilarity(tfidfDataArray,tfidfDataTestArray,keyval,n_neighbors):\n","  \n","  allDistances = []\n","  for key in tfidfDataArray:\n","    #print(key)\n","    list1 = np.array(tfidfDataArray[key])\n","    list2 = np.array(tfidfDataTestArray[keyval])\n","    list1Val = np.sqrt(np.sum((list1)**2))\n","    list2Val = np.sqrt(np.sum((list2)**2))\n","    #print(list1)\n","    #print(list2)\n","    allDistances.append((key,np.dot(list1,list2)/(list1Val*list2Val)))\n","\n","  allDistances.sort(key=lambda x: x[1],reverse=True)\n","  voting_count = {}\n","  count = 0\n","  for val in allDistances:\n","    if count==n_neighbors:\n","      break\n","    if val[0][0] in voting_count:\n","      voting_count[val[0][0]] += 1\n","    else :\n","      voting_count[val[0][0]] = 1\n","    count += 1\n","  predicted = max(voting_count, key=voting_count.get)\n","  return predicted"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXPPMylJyEZj","executionInfo":{"status":"ok","timestamp":1602602883756,"user_tz":-360,"elapsed":2235586,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"96189ac8-d34a-4731-a628-5cfd346adde0","colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["def performanceEvaluationKNN(trainDataStore,validdataStore,position_array,topic_list):\n","  \n","  binary_represent_final_train,uniqueWord,binary_represent_final_test,bow_represent_final_train,bow_represent_final_test = BinaryRepresent(trainDataStore,position_array,topic_list,validdataStore)\n","  #print(bow_represent_final_train)\n","  print(\"bow represent created\")\n","  ####################\n","  \n","  header = list(binary_represent_final_train.keys())\n","  testingBinArr = np.array(list(binary_represent_final_train.values()))\n","  #tfidfDataArray,tfidfDataTestArray = calculateTFIDFProcess(trainDataStore,topic_list,validdataStore,uniqueWord)\n","  #print(len(binary_represent_final_train))\n","  #print(binary_represent_final_test)\n","  #print(bow_represent_final_test)\n","\n","  headerBow = list(bow_represent_final_train.keys())\n","  testingBinArrBow = np.array(list(bow_represent_final_train.values()))\n","  \n","  \n","  # binary representation\n","  print(\"bin training started\")\n","  allBinDisctance = {}\n","  for dataName in binary_represent_final_test:\n","      predicted = predictionBinary(header,testingBinArr,binary_represent_final_test,dataName)\n","      allBinDisctance[dataName] = predicted\n","\n","  n_neighbors = [1,3,5]\n","  for neighbor in n_neighbors:\n","    print(\"Neighbor :\",neighbor)\n","    totalCount = 0\n","    correctCount = 0\n","    for dataName in binary_represent_final_test:\n","      predArray = allBinDisctance[dataName]\n","      voting_count = {}\n","      for val in predArray[:neighbor]:\n","        if val[0] in voting_count:\n","          voting_count[val[0]] += 1\n","        else :\n","          voting_count[val[0]] = 1\n","\n","      prediction = max(voting_count, key=voting_count.get)\n","      if prediction == dataName[0]:\n","        correctCount += 1\n","      totalCount += 1\n","      \n","    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","  print()\n","  \n","  # bag of words\n","  print(\"bow training started\")\n","  allBinDisctance = {}\n","  for dataName in bow_represent_final_test:\n","      predicted = predictionBow(headerBow,testingBinArrBow,bow_represent_final_test,dataName)\n","      allBinDisctance[dataName] = predicted\n","\n","  n_neighbors = [1,3,5]\n","  for neighbor in n_neighbors:\n","    print(\"Neighbor :\",neighbor)\n","    totalCount = 0\n","    correctCount = 0\n","    for dataName in bow_represent_final_test:\n","      predArray = allBinDisctance[dataName]\n","      voting_count = {}\n","      for val in predArray[:neighbor]:\n","        if val[0] in voting_count:\n","          voting_count[val[0]] += 1\n","        else :\n","          voting_count[val[0]] = 1\n","\n","      prediction = max(voting_count, key=voting_count.get)\n","      if prediction == dataName[0]:\n","        correctCount += 1\n","      totalCount += 1\n","      \n","    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","  print()\n","  \n","  '''\n","  # tf-idf\n","  n_neighbors = [1,3,5]\n","  for neighbor in n_neighbors:\n","    print(\"Neighbor :\",neighbor)\n","    totalCount = 0\n","    correctCount = 0\n","    for keyval in tfidfDataTestArray:\n","      predicted = calculateCosineSimilarity(tfidfDataArray,tfidfDataTestArray,keyval,n_neighbors)\n","      if predicted == keyval[0]:\n","        correctCount += 1\n","      totalCount += 1\n","    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","  print()\n","  '''\n","performanceEvaluationKNN(trainDataStore,validdataStore,position_array,topic_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bow represent created\n","bin training started\n","Neighbor : 1\n","Total Correct Count:  703  Total Wrong Count:  1404  Accuracy:  33.36497389653536\n","Neighbor : 3\n","Total Correct Count:  733  Total Wrong Count:  1374  Accuracy:  34.78879924062648\n","Neighbor : 5\n","Total Correct Count:  768  Total Wrong Count:  1339  Accuracy:  36.44992880873279\n","\n","bow training started\n","Neighbor : 1\n","Total Correct Count:  1134  Total Wrong Count:  973  Accuracy:  53.820598006644516\n","Neighbor : 3\n","Total Correct Count:  1156  Total Wrong Count:  951  Accuracy:  54.864736592311345\n","Neighbor : 5\n","Total Correct Count:  1159  Total Wrong Count:  948  Accuracy:  55.007119126720454\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kf0GviDnPGCl","executionInfo":{"status":"ok","timestamp":1602602883758,"user_tz":-360,"elapsed":2235500,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"0611fac2-12fe-4e02-edb3-1a2593537b11","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["list1 = np.array([2,3])\n","list2 = np.array([1,2])\n","list3 = np.array([[]])\n","list3 = np.append(list3,[list1],axis=1)\n","list3 = np.append(list3,[list2],axis=0)\n","#list1 = list1.insert([1,2],axis=0)\n","#list1 = np.insert(0,[1,2])\n","list4 = np.array([1,4])\n","np.sum(np.subtract(list3,list4),axis=1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0., -2.])"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"id":"ak_cftt7lxc1","executionInfo":{"status":"ok","timestamp":1602602883758,"user_tz":-360,"elapsed":2235475,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"7237201b-9e91-409e-ba95-ad14568e6d8b","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["import numpy as np\n","dictionary = {\"raj\": np.array([1,2]), \"striver\": np.array([2,3]), \"vikram\": np.array([3,4])} \n","#print(dictionary.values())\n","list1 = list(dictionary.keys())\n","list3 =np.array(list(dictionary.values()))   \n","print(list3)\n","list4 = np.array([1,4])\n","list5 = list(abs(np.sum(np.subtract(list3,list4),axis=1)))\n","print(list5)\n","print(list1)\n","list2, list1 = zip(*sorted(zip(list5, list1)))\n","print(list1[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1 2]\n"," [2 3]\n"," [3 4]]\n","[2, 0, 2]\n","['raj', 'striver', 'vikram']\n","striver\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pfjy0jys5h15","executionInfo":{"status":"ok","timestamp":1602602883759,"user_tz":-360,"elapsed":2235443,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"d325af49-eea9-4ce6-f35d-57c2b03bc7bd","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["dictionary = {\"raj\": np.array([1.1,2.1]), \"striver\": np.array([2.5,3.6]), \"vikram\": np.array([3.6,4.9])} \n","#print(dictionary.values())\n","list1 = list(dictionary.keys())\n","list3 =np.array(list(dictionary.values()))   \n","print(list3)\n","list4 = np.array([1,4.1])\n","distance = list(np.sqrt(np.sum(np.square(np.subtract(list3,list4)),axis=1)))\n","print(type(distance))\n","print(distance)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1.1 2.1]\n"," [2.5 3.6]\n"," [3.6 4.9]]\n","<class 'list'>\n","[2.002498439450078, 1.5811388300841895, 2.720294101747089]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Adfs_VojXgFU","executionInfo":{"status":"ok","timestamp":1602602883761,"user_tz":-360,"elapsed":2235408,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"bd5d6391-b45d-483e-b1ee-ec88312f6621","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","dictionary = {\"raj\": np.array([1,2]), \"striver\": np.array([2,3]), \"vikram\": np.array([3,4])} \n","list1 = list(dictionary.keys())\n","list3 =np.array(list(dictionary.values()))\n","list4 = np.array([1,4])\n","listVal1 = np.sqrt(np.sum(np.square(list3),axis=1))\n","listVal2 = np.sqrt(np.sum(np.square(list4)))\n","dotResult = np.dot(list3,list4)\n","finalResult = np.divide(dotResult,np.dot(listVal1,listVal2))\n","finalResult"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.97618706, 0.94174191, 0.92163538])"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"id":"zP5qlnL8UeWX","executionInfo":{"status":"ok","timestamp":1602602883761,"user_tz":-360,"elapsed":2235375,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"eecb3d75-c42f-4aa4-d0cf-7e2d65746f96","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["import numpy as np\n","dictionary = {\"raj\": np.array([1,2]), \"striver\": np.array([2,3]), \"vikram\": np.array([3,4])} \n","#print(dictionary.values())\n","list1 = list(dictionary.keys())\n","list3 =np.array(list(dictionary.values()))   \n","print(list3)\n","list4 = np.array([1,4])\n","\n","from scipy.spatial import distance\n","list(distance.cdist(list3,[list4],'cityblock').flatten())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1 2]\n"," [2 3]\n"," [3 4]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[2.0, 2.0, 2.0]"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"code","metadata":{"id":"TEnAtZcGIHis","executionInfo":{"status":"ok","timestamp":1602755691813,"user_tz":-360,"elapsed":9306972,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"49b14619-ccb6-4a33-dbf4-663e831e01c8","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip3 install bs4\n","!pip install spacy\n","\n","import nltk\n","import math\n","import os\n","import fnmatch\n","import random\n","import numpy as np\n","from xml.etree import ElementTree\n","import re\n","import string\n","import spacy\n","from scipy.spatial import distance\n","from nltk.corpus import stopwords          \n","from nltk.stem import PorterStemmer        \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from bs4 import BeautifulSoup as bs\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","sp = spacy.load('en_core_web_sm')\n","spacy_stopwords = sp.Defaults.stop_words\n","\n","def preprocessText(text):\n","\n","  def makeTextLower(text):\n","    result = text.lower() \n","    return result \n","\n","  def MakeNumberRemove(text): \n","    #result = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', text)\n","    result = re.sub(r'[-+]?\\d+', '', text) \n","    return result \n","\n","  def MakeHTMLremove(text):\n","    cleancode = re.compile('<code>.*?</code>')\n","    cleanr = re.compile('<.*?>')\n","    cleanentity = re.compile('&.*;')\n","    cleantext = re.sub(cleancode, '', text)\n","    cleantext = re.sub(cleanr, ' ', cleantext)\n","    cleantext = re.sub(cleanentity, ' ', cleantext)  \n","    return cleantext\n","\n","  def makeRemoveExtraApace(text):\n","    result = \" \".join(text.split())  \n","    return result\n","\n","  def makeRemoveHyperLink(text):\n","    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","    return result  \n","\n","  def makeRemovePunctuation(text):\n","    result = text=text.translate((str.maketrans('','',string.punctuation)))\n","    return result\n","  \n","  def makeWordLemmatize(text):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = [lemmatizer.lemmatize(word,pos='v') for word in text] \n","    return lemmas\n","\n","  def removeStopWords(text):\n","    stopwords_english = stopwords.words('english')\n","    texts_clean = []\n","    #print(text)\n","    text = word_tokenize(text)\n","    for word in text: \n","      if word not in stopwords_english:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def removeSpacyStopWords(text,spacy_stopwords):\n","    texts_clean = []\n","    #print(text)\n","    for word in text: \n","      if word not in spacy_stopwords:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def makeStemming(text):\n","    ps = PorterStemmer() \n","    #words = word_tokenize(text)\n","    lemmas = [ps.stem(word) for word in text] \n","    return lemmas  \n","\n","  result = text\n","  result=re.sub(r\"<.+?>\",'',result)\n","  result=re.sub(r'[^\\x00-\\x7F]+','',result)\n","  result = MakeHTMLremove(result)\n","  result = makeTextLower(result)\n","  result = MakeNumberRemove(result)\n","  result = makeRemoveHyperLink(result)\n","  result = makeRemovePunctuation(result)\n","  result = removeStopWords(result)\n","  result = removeSpacyStopWords(result,spacy_stopwords)\n","  result = makeWordLemmatize(result)\n","  result = makeStemming(result)\n","  return result\n","\n","def runTFIDF(datastore,validstore,topic_list,topicLength,type):\n","\n","  def numberDocsForWord(datastore,words):\n","    wordVal = {}\n","    value = list(datastore.values())\n","    for word in words:\n","      count = sum(word in e for e in value)    \n","      wordVal[word] = count\n","    return wordVal\n","\n","  def uniqueWordFunc(trainDataStore,testStore):\n","    uniqueWord = []\n","    for pair in trainDataStore:\n","      wordList = trainDataStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    wordval = numberDocsForWord(datastore,uniqueWord)\n","    for pair in testStore:\n","      wordList = testStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    return uniqueWord,wordval\n","\n","  def countData(datastore,validstore,uniqueWord):\n","    countDatastore = {}\n","    countDatastoreTest = {}\n","    \n","    for key in datastore:\n","      tempArray = list(datastore[key])\n","      wordDictA = []\n","      for word in uniqueWord:\n","        wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","      countDatastore[key] = wordDictA\n","      \n","    for key in validstore:\n","      tempArray = list(validstore[key])\n","      wordDictA = []\n","      for word in uniqueWord:\n","        wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","      countDatastoreTest[key] = wordDictA\n","    \n","    return countDatastore,countDatastoreTest\n","\n","  uniqueWord,wordval = uniqueWordFunc(datastore,validstore)    \n","  tfData,tfDataTest = countData(datastore,validstore,uniqueWord)\n","  print(len(uniqueWord))\n","\n","  def computeIDF(datastore,validstore,numberofDoc,dictionaryWords,wordval,tfData,tfDataTest):\n","    \n","    idfDict = {}\n","    idfDict = dict.fromkeys(set(dictionaryWords), 0)\n","    \n","    for key in datastore:\n","      tempArray = datastore[key]\n","      temp = []\n","      for word in dictionaryWords:\n","        if word in tempArray:\n","          temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","        else :\n","          temp.append(0.0)\n","      tfData[key] = list(np.multiply(temp,tfData[key]))\n","\n","    for key in validstore:\n","      tempArray = validstore[key]\n","      temp = []\n","      for word in dictionaryWords:\n","        if word in tempArray and word in wordval:\n","          temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","        else :\n","          temp.append(0.0)\n","      tfDataTest[key] = list(np.multiply(temp,tfDataTest[key]))\n","        \n","    return tfData,tfDataTest\n","\n","  numberofDoc = len(datastore)\n","  #print(numberofDoc)\n","\n","  idfData = {}\n","  tfidfDataArray,tfidfDataTestArray = computeIDF(datastore,validstore,numberofDoc,uniqueWord,wordval,tfData,tfDataTest)\n","  \n","  def cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval):\n","    listVal1 = np.sqrt(np.sum(np.square(testingBinArrCS),axis=1))\n","    listVal2 = np.sqrt(np.sum(np.square(tfidfDataTestArray[keyval])))\n","    dotResult = np.dot(testingBinArrCS,tfidfDataTestArray[keyval])\n","    finalResult = np.divide(dotResult,np.dot(listVal1,listVal2))\n","    finalResult, headerCS = (list(t) for t in zip(*sorted(zip(finalResult, headerCS),reverse=True)))\n","    return headerCS\n","\n","  headerCS = list(tfidfDataArray.keys())\n","  testingBinArrCS = np.array(list(tfidfDataArray.values()))\n","\n","  allBinDisctance = {}\n","  for keyval in tfidfDataTestArray:\n","    predicted = cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval)\n","    allBinDisctance[keyval] = predicted\n","\n","  if type is 'val':\n","    n_neighbors = [1,3,5]\n","  elif type is 'test':\n","    n_neighbors = [5]\n","\n","  for neighbor in n_neighbors:\n","    print(\"Neighbor :\",neighbor)\n","    totalCount = 0\n","    correctCount = 0\n","    for keyval in tfidfDataTestArray:\n","      predArray = allBinDisctance[keyval]\n","      voting_count = {}\n","      for val in predArray[:neighbor]:\n","        if val[0] in voting_count:\n","          voting_count[val[0]] += 1\n","        else :\n","          voting_count[val[0]] = 1\n","\n","      prediction = max(voting_count, key=voting_count.get)\n","      if prediction == keyval[0]:\n","        correctCount += 1\n","      totalCount += 1\n","      \n","    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","  print()\n","  return correctCount/float(totalCount)\n","\n","\n","\n","train_path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"\n","#topic name reading\n","topic_number =0\n","topic_list = {}\n","with open(\"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/topics.txt\") as fp:\n","  lines = fp.readlines()\n","   \n","  for line in lines:\n","    line = line.strip()\n","    topic_list[line] = topic_number\n","    topic_number += 1\n","    '''\n","    if topic_number == 3 :\n","      break\n","    '''\n","fp.close()\n","print(topic_list)\n","topicLength = len(topic_list)\n","# train data and validation data \n","topicDataSize = 700\n","trainDataSize = 500\n","validationDataSize = 200\n","allResult = {}\n","trainDataStore = {}\n","validdataStore = {}\n","count = 0\n","\n","position_array = []\n","for i in range(0,topicDataSize):\n","  position_array.append(i)\n","\n","for topic in topic_list:\n","  \n","  with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","    content = file.read()\n","    soup = bs(content)\n","    rowList = soup.findAll(\"row\")\n","    count = 0\n","    for row in rowList[0:trainDataSize]:\n","      topicPair = (topic,position_array[count])\n","      if len(row.get('body')) == 0:\n","        continue;\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      for word in word_list:\n","        pairWord = (word,topic_list[topic])\n","        if pairWord in allResult:\n","          allResult[pairWord] += 1\n","        else:\n","          allResult[pairWord] = 1\n","      count += 1\n","      trainDataStore[topicPair] = np.array(word_list)\n","\n","    #validation data\n","    for i in range(trainDataSize,trainDataSize+validationDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      validdataStore[topicPair] = np.array(word_list)\n","\n","print(\"Document count :\",count)\n","\n","totalLoop = 50\n","acc = []\n","for loop in range(40,50):\n","  testSize = 10\n","  position_array = []\n","  for i in range(topicDataSize+loop*testSize,topicDataSize+loop*testSize+testSize):\n","    position_array.append(i)\n","  testDatastore = {}\n","\n","  for topic in topic_list:\n","    with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","      content = file.read()\n","      soup = bs(content)\n","      rowList = soup.findAll(\"row\")\n","      \n","      for i in range(len(position_array)):\n","        topicPair = (topic,position_array[i])\n","        row = rowList[position_array[i]]\n","        if len(row.get('body')) == 0:\n","          continue\n","        word_list = preprocessText(row.get('body'))\n","        if len(word_list) == 0:\n","          continue\n","        testDatastore[topicPair] = np.array(word_list) \n","\n","  accuracy = runTFIDF(trainDataStore,testDatastore,topic_list,topicLength,'test')\n","  acc.append(accuracy)\n","\n","print(acc)\n","print(len(acc))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","{'Coffee': 0, 'Arduino': 1, 'Anime': 2, 'Astronomy': 3, 'Biology': 4, 'Chess': 5, 'Cooking': 6, 'Law': 7, 'Space': 8, 'Windows_Phone': 9, 'Wood_Working': 10}\n","Document count : 500\n","19523\n","Neighbor : 5\n","Total Correct Count:  90  Total Wrong Count:  17  Accuracy:  84.11214953271028\n","\n","19551\n","Neighbor : 5\n","Total Correct Count:  86  Total Wrong Count:  15  Accuracy:  85.14851485148515\n","\n","19604\n","Neighbor : 5\n","Total Correct Count:  91  Total Wrong Count:  19  Accuracy:  82.72727272727273\n","\n","19519\n","Neighbor : 5\n","Total Correct Count:  91  Total Wrong Count:  16  Accuracy:  85.04672897196262\n","\n","19589\n","Neighbor : 5\n","Total Correct Count:  89  Total Wrong Count:  18  Accuracy:  83.17757009345794\n","\n","19507\n","Neighbor : 5\n","Total Correct Count:  87  Total Wrong Count:  23  Accuracy:  79.0909090909091\n","\n","19481\n","Neighbor : 5\n","Total Correct Count:  88  Total Wrong Count:  17  Accuracy:  83.80952380952381\n","\n","19537\n","Neighbor : 5\n","Total Correct Count:  94  Total Wrong Count:  16  Accuracy:  85.45454545454545\n","\n","19532\n","Neighbor : 5\n","Total Correct Count:  92  Total Wrong Count:  17  Accuracy:  84.40366972477064\n","\n","19462\n","Neighbor : 5\n","Total Correct Count:  85  Total Wrong Count:  21  Accuracy:  80.18867924528301\n","\n","[0.8411214953271028, 0.8514851485148515, 0.8272727272727273, 0.8504672897196262, 0.8317757009345794, 0.7909090909090909, 0.8380952380952381, 0.8545454545454545, 0.8440366972477065, 0.8018867924528302]\n","10\n"],"name":"stdout"}]}]}