{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1505044_naive.ipynb","provenance":[],"mount_file_id":"1Te2mzrMllxP6dfEnG2flWuiadSwRjZ4r","authorship_tag":"ABX9TyM9e/YNWjwbQhWDSDpBWpwc"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"voPBcAMoKpHW","executionInfo":{"status":"ok","timestamp":1602595531281,"user_tz":-360,"elapsed":7305,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"785715ef-c776-497f-dcff-bd18a709136c","colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["!pip3 install bs4\n","!pip install spacy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"snmOX3WyBgRs"},"source":["import nltk\n","import math\n","import os\n","import fnmatch\n","import random\n","import numpy as np\n","from xml.etree import ElementTree\n","import re\n","import string\n","import spacy\n","from nltk.corpus import stopwords          \n","from nltk.stem import PorterStemmer        \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","from bs4 import BeautifulSoup as bs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7OrNK-CEIOw","executionInfo":{"status":"ok","timestamp":1602595534682,"user_tz":-360,"elapsed":10678,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"7309a24b-4d4e-4373-a854-caea2b1e3df7","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# download the stopwords from NLTK\n","#nltk.download('all')\n","# download the stopwords from NLTK\n","#nltk.download('all')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"k9bmr9u3EhjO"},"source":["def makeTextLower(text):\n","  result = text.lower() \n","  return result "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gccnY93EEowm"},"source":["def MakeNumberRemove(text): \n","  #result = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', text)\n","  result = re.sub(r'[-+]?\\d+', '', text) \n","  return result "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXvVsv0cEr5_"},"source":["def MakeHTMLremove(text):\n","  \n","  cleancode = re.compile('<code>.*?</code>')\n","  cleanr = re.compile('<.*?>')\n","  cleanentity = re.compile('&.*;')\n","  cleantext = re.sub(cleancode, '', text)\n","  cleantext = re.sub(cleanr, ' ', cleantext)\n","  cleantext = re.sub(cleanentity, ' ', cleantext)\n","  \n","  return cleantext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ELJRi0BOEuNw"},"source":["def makeRemoveExtraApace(text):\n","  result = \" \".join(text.split())  \n","  return result   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZB6gdVDExDy"},"source":["def makeRemoveHyperLink(text):\n","  result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0h_ThUBEzk1"},"source":["def makeRemovePunctuation(text):\n","  \n","  #print(string.punctuation)\n","  #translator = str.maketrans('', '', string.punctuation)\n","  result = text=text.translate((str.maketrans('','',string.punctuation)))\n","  return result\n","  \n","  '''\n","  puncts = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n","\n","  for punct in puncts:\n","    if punct in text:\n","      text = text.replace(punct, '')\n","\n","  return text\n","  '''\n","#makeRemovePunctuation(\"Hi;\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIagPNV1E4I1"},"source":["def makeWordLemmatize(text):\n","  lemmatizer = WordNetLemmatizer()\n","  #word_tokens = word_tokenize(text)\n","  lemmas = [lemmatizer.lemmatize(word,pos='v') for word in text] \n","  return lemmas "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yetF8QwBSpsf"},"source":["def cleanTextData(text):\n","\n","  #uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n","  text = text.lower()\n","  # specific\n","  text = re.sub(r\"won't\", \"will not\", text)\n","  text = re.sub(r\"can\\'t\", \"can not\", text)\n","\n","  # general\n","  text = re.sub(r\"n\\'t\", \" not\", text)\n","  text = re.sub(r\"\\'re\", \" are\", text)\n","  text = re.sub(r\"\\'s\", \" is\", text)\n","  text = re.sub(r\"\\'d\", \" would\", text)\n","  text = re.sub(r\"\\'ll\", \" will\", text)\n","  text = re.sub(r\"\\'t\", \" not\", text)\n","  text = re.sub(r\"\\'ve\", \" have\", text)\n","  text = re.sub(r\"\\'m\", \" am\", text)\n","  text = re.sub(r\"\\n\", \"\", text)\n","  #text = re.sub(uri_re, \"\", text)\n","  text = re.sub(r\"what's\", \"what is \", text)\n","  text = re.sub(r\"\\'s\", \" \", text)\n","  text = re.sub(r\"\\'ve\", \" have \", text)\n","  text = re.sub(r\"can't\", \"can not \", text)\n","  text = re.sub(r\"n't\", \" not \", text)\n","  text = re.sub(r\"i'm\", \"i am \", text)\n","  text = re.sub(r\"\\'re\", \" are \", text)\n","  text = re.sub(r\"\\'d\", \" would \", text)\n","  text = re.sub(r\"\\'ll\", \" will \", text)\n","  text = re.sub(r\"\\'scuse\", \" excuse \", text)\n","  text = re.sub(r\"\\'\\n\", \" \", text)\n","  text = re.sub(r\"\\'\\xa0\", \" \", text)\n","  text = re.sub('\\s+', ' ', text)\n","  text = text.strip(' ')\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MM6hQ8WE7n6"},"source":["def removeStopWords(text):\n","  stopwords_english = stopwords.words('english')\n","  texts_clean = []\n","  #print(text)\n","  text = word_tokenize(text)\n","  for word in text: \n","    if word not in stopwords_english:  \n","        texts_clean.append(word)\n","  return texts_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzPbn_m_aIOV"},"source":["sp = spacy.load('en_core_web_sm')\n","spacy_stopwords = sp.Defaults.stop_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_wt1Jn5YiJn"},"source":["def removeSpacyStopWords(text,spacy_stopwords):\n","  \n","  texts_clean = []\n","  #print(text)\n","  for word in text: \n","    if word not in spacy_stopwords:  \n","        texts_clean.append(word)\n","  return texts_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAX1KGfwdF6z"},"source":["def makeStemming(text):\n","  ps = PorterStemmer() \n","  #words = word_tokenize(text)\n","  lemmas = [ps.stem(word) for word in text] \n","  return lemmas  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wCzRBOSE-s5"},"source":["def preprocessText(text):\n","\n","  '''\n","  - html remove\n","  - Transforming abbreviations\n","  - Removing punctuation\n","  - Lemmatizing/stemming words ?\n","  - Removing stop words\n","  '''\n","  #result=re.sub(r\"<a.*</a>\", '', text)\n","  result = text\n","  result=re.sub(r\"<.+?>\",'',result)\n","  #result = makeTextLower(text)\n","  result=re.sub(r'[^\\x00-\\x7F]+','',result)\n","  result = MakeHTMLremove(result)\n","  #result = makeRemoveExtraApace(result)\n","  result = makeTextLower(result)\n","  \n","  #result = cleanTextData(result)\n","  result = MakeNumberRemove(result)\n","  result = makeRemoveHyperLink(result)\n","  result = makeRemovePunctuation(result)\n","  \n","  result = removeStopWords(result)\n","  result = removeSpacyStopWords(result,spacy_stopwords)\n","  result = makeWordLemmatize(result)\n","  result = makeStemming(result)\n","  \n","  \n","  #result = makeTextLower(result)\n","\n","  return result  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eo0lxPGHFG9u","executionInfo":{"status":"ok","timestamp":1602595615540,"user_tz":-360,"elapsed":91462,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"1383cf4f-62c7-46a0-8c3e-696cb6f9f7b7","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["train_path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"\n","#topic name reading\n","topic_number =0\n","topic_list = {}\n","with open(\"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/topics.txt\") as fp:\n","  lines = fp.readlines()\n","   \n","  for line in lines:\n","    line = line.strip()\n","    topic_list[line] = topic_number\n","    topic_number += 1\n","    '''\n","    if topic_number == 3 :\n","      break\n","    '''\n","fp.close()\n","print(topic_list)\n","topicLength = len(topic_list)\n","#dom = ElementTree.parse(train_path+xml_file[0])\n","#topic_list = {\"Anime\":0,\"Arduino\":1,\"Biology\":2,\"Chess\":3}\n","#topicLength = len(topic_list)\n","#print(topicLength)\n","#each topic data size\n","\n","# train data and validation data \n","topicDataSize = 510\n","trainDataSize = 500\n","validationDataSize = 10\n","allResult = {}\n","count = 0\n","for topic in topic_list:\n","  #print(topic)\n","  \n","  '''\n","  dom = ElementTree.parse(train_path+topic+\".xml\")\n","  rowList = dom.findall(\"row\")\n","\n","  topic_sentence = {}\n","\n","  for row in rowList[0:topicDataSize]:\n","    #print(row.get('Body'))\n","    #print()\n","    #print(preprocessText(row.get('Body')))\n","    word_list = preprocessText(row.get('Body'))\n","    for word in word_list:\n","      pairWord = (word,topic_list[topic])\n","      if pairWord in allResult:\n","        allResult[pairWord] += 1\n","      else:\n","        allResult[pairWord] = 1\n","  '''\n","  with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","    content = file.read()\n","    soup = bs(content)\n","    rowList = soup.findAll(\"row\")\n","    for row in rowList[0:trainDataSize]:\n","      #print(row.get('body'))\n","      #print()\n","      #print(preprocessText(row.get('Body')))\n","      if len(row.get('body')) == 0:\n","        continue;\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      for word in word_list:\n","        pairWord = (word,topic_list[topic])\n","        if pairWord in allResult:\n","          allResult[pairWord] += 1\n","        else:\n","          allResult[pairWord] = 1\n","      count += 1\n","print(\"Document count :\",count)\n","#print(allResult)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'Coffee': 0, 'Arduino': 1, 'Anime': 2, 'Astronomy': 3, 'Biology': 4, 'Chess': 5, 'Cooking': 6, 'Law': 7, 'Space': 8, 'Windows_Phone': 9, 'Wood_Working': 10}\n","Document count : 5280\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5YiyDu50I8Oo"},"source":["def trainNaive(allResult,topicLength,smFactor):\n","  \n","  uniqueWord = []\n","  for word in allResult.keys():\n","    if word[0] not in uniqueWord:\n","      uniqueWord.append(word[0])\n","  uniqueWordSize = len(uniqueWord)\n","  #print(uniqueWord[:50])\n","\n","  topicWiseCount = {}\n","  for wordPair in allResult.keys():\n","    if wordPair in topicWiseCount.keys():\n","      topicWiseCount[wordPair[1]] = topicWiseCount[wordPair[1]] + allResult[wordPair]\n","    else :\n","      topicWiseCount[wordPair[1]] = 1\n","\n","  laplaceSmoothingCalc = {}\n","  for word in uniqueWord:\n","    for topic in range(topicLength):\n","      \n","      wordPair = (word,topic)\n","      prevCount = 0\n","      if wordPair in allResult:\n","        prevCount = allResult[wordPair]\n","      probability = (prevCount+smFactor)/(float(topicWiseCount[topic])+smFactor*uniqueWordSize)\n","\n","      laplaceSmoothingCalc[wordPair] = probability\n","  \n","  return topicWiseCount,laplaceSmoothingCalc,uniqueWordSize"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hJM7QN3QzP-","executionInfo":{"status":"ok","timestamp":1602595619389,"user_tz":-360,"elapsed":95235,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"2e0aabdd-0363-4184-c8ae-d698103faf1d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["topicWiseCount,laplaceSmoothingCalc,uniqueWordSize = trainNaive(allResult,topicLength,1)\n","print(uniqueWordSize)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["19237\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xnj6HCglRsew"},"source":["def predictNaive(word_list,laplaceSmoothingCalc,topicLength,topicWiseCount,uniqueWordSize,smFactor):\n","  \n","  #word_list = preprocessText(document)\n","  \n","  maxProb = 0.0\n","  currTopic = 0\n","  allDistances = []\n","  for topic in range(topicLength):\n","    multiProb = math.log(1/float(topicLength))\n","    for word in word_list:\n","      wordPair = (word,topic)\n","      if wordPair in laplaceSmoothingCalc.keys():\n","        multiProb = multiProb + math.log(laplaceSmoothingCalc[wordPair])\n","      else :\n","        multiProb = multiProb + math.log((smFactor)/(topicWiseCount[topic]+smFactor*uniqueWordSize))\n","        #continue\n","    #print(multiProb)\n","    allDistances.append((topic,word,multiProb))\n","    '''\n","    if multiProb > maxProb:\n","      maxProb = multiProb\n","      currTopic = topic\n","      #print(currTopic)\n","    '''\n","  allDistances.sort(key=lambda x: x[2],reverse=True)\n","  currTopic = allDistances[0][0]\n","  #print()\n","  return currTopic\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuqJESHDXLNQ","executionInfo":{"status":"ok","timestamp":1602595887276,"user_tz":-360,"elapsed":363084,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"e65a5903-bfc0-457d-b1f8-76e11580f89a","colab":{"base_uri":"https://localhost:8080/","height":629}},"source":["smfactorList = [.001,.005,.01,.05,.08,0.1,0.3,0.5,0.8,1,10,20]\n","for smFactor in smfactorList:\n","  print(\"smoothing factor constant :\",smFactor)\n","  topicWiseCount,laplaceSmoothingCalc,uniqueWordSize = trainNaive(allResult,topicLength,smFactor)\n","  #print(laplaceSmoothingCalc[(\"get\",1)])\n","  #print(uniqueWordSize)\n","  correctCount = 0\n","  totalCount = 0\n","  for key in topic_list:\n","    path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"+key+\".xml\"\n","    #print(key)\n","    \n","    with open(path,'r',encoding='utf-8') as file:\n","      \n","      content = file.read()\n","      soup = bs(content)\n","      rowList = soup.findAll(\"row\")\n","      #correctCount = 0\n","      for row in rowList[500:701]:\n","        #print(row.get('body'))\n","        #print()\n","        if len(row.get('body')) == 0:\n","          continue\n","        word_list = preprocessText(row.get('body'))\n","        \n","        if len(word_list) == 0:\n","          continue\n","        \n","        if predictNaive(word_list,laplaceSmoothingCalc,topicLength,topicWiseCount,uniqueWordSize,smFactor) is topic_list[key]:\n","          correctCount += 1\n","        totalCount += 1\n","  print((correctCount/float(totalCount))*100)\n","  print()    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["smoothing factor constant : 0.001\n","87.72426817752597\n","\n","smoothing factor constant : 0.005\n","87.91312559017942\n","\n","smoothing factor constant : 0.01\n","88.0547686496695\n","\n","smoothing factor constant : 0.05\n","88.24362606232295\n","\n","smoothing factor constant : 0.08\n","88.14919735599622\n","\n","smoothing factor constant : 0.1\n","88.10198300283287\n","\n","smoothing factor constant : 0.3\n","87.62983947119925\n","\n","smoothing factor constant : 0.5\n","87.48819641170917\n","\n","smoothing factor constant : 0.8\n","87.11048158640227\n","\n","smoothing factor constant : 1\n","86.96883852691218\n","\n","smoothing factor constant : 10\n","84.98583569405099\n","\n","smoothing factor constant : 20\n","83.80547686496695\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nsxbs_bSgjRp","executionInfo":{"status":"ok","timestamp":1602739734831,"user_tz":-360,"elapsed":567748,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"00e2e146-d9b3-4cc1-d98a-a6ea5b2139ea","colab":{"base_uri":"https://localhost:8080/","height":598}},"source":["!pip3 install bs4\n","!pip install spacy\n","\n","import nltk\n","import math\n","import os\n","import fnmatch\n","import random\n","import numpy as np\n","from xml.etree import ElementTree\n","import re\n","import string\n","import spacy\n","from scipy.spatial import distance\n","from nltk.corpus import stopwords          \n","from nltk.stem import PorterStemmer        \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from bs4 import BeautifulSoup as bs\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","sp = spacy.load('en_core_web_sm')\n","spacy_stopwords = sp.Defaults.stop_words\n","\n","def preprocessText(text):\n","\n","  def makeTextLower(text):\n","    result = text.lower() \n","    return result \n","\n","  def MakeNumberRemove(text): \n","    #result = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', text)\n","    result = re.sub(r'[-+]?\\d+', '', text) \n","    return result \n","\n","  def MakeHTMLremove(text):\n","    cleancode = re.compile('<code>.*?</code>')\n","    cleanr = re.compile('<.*?>')\n","    cleanentity = re.compile('&.*;')\n","    cleantext = re.sub(cleancode, '', text)\n","    cleantext = re.sub(cleanr, ' ', cleantext)\n","    cleantext = re.sub(cleanentity, ' ', cleantext)  \n","    return cleantext\n","\n","  def makeRemoveExtraApace(text):\n","    result = \" \".join(text.split())  \n","    return result\n","\n","  def makeRemoveHyperLink(text):\n","    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","    return result  \n","\n","  def makeRemovePunctuation(text):\n","    result = text=text.translate((str.maketrans('','',string.punctuation)))\n","    return result\n","  \n","  def makeWordLemmatize(text):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = [lemmatizer.lemmatize(word,pos='v') for word in text] \n","    return lemmas\n","\n","  def removeStopWords(text):\n","    stopwords_english = stopwords.words('english')\n","    texts_clean = []\n","    #print(text)\n","    text = word_tokenize(text)\n","    for word in text: \n","      if word not in stopwords_english:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def removeSpacyStopWords(text,spacy_stopwords):\n","    texts_clean = []\n","    #print(text)\n","    for word in text: \n","      if word not in spacy_stopwords:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def makeStemming(text):\n","    ps = PorterStemmer() \n","    #words = word_tokenize(text)\n","    lemmas = [ps.stem(word) for word in text] \n","    return lemmas  \n","\n","  result = text\n","  result=re.sub(r\"<.+?>\",'',result)\n","  result=re.sub(r'[^\\x00-\\x7F]+','',result)\n","  result = MakeHTMLremove(result)\n","  result = makeTextLower(result)\n","  result = MakeNumberRemove(result)\n","  result = makeRemoveHyperLink(result)\n","  result = makeRemovePunctuation(result)\n","  result = removeStopWords(result)\n","  result = removeSpacyStopWords(result,spacy_stopwords)\n","  result = makeWordLemmatize(result)\n","  result = makeStemming(result)\n","  return result  \n","\n","def naiveBayes(allResult,trainDataSize,validationDataSize,topic_list,topicLength,smfactorList,type):\n","  trainDataSize = trainDataSize\n","  validationDataSize = validationDataSize\n","  def trainNaive(allResult,topicLength,smFactor):\n","  \n","    uniqueWord = []\n","    for word in allResult.keys():\n","      if word[0] not in uniqueWord:\n","        uniqueWord.append(word[0])\n","    uniqueWordSize = len(uniqueWord)\n","    #print(uniqueWord[:50])\n","\n","    topicWiseCount = {}\n","    for wordPair in allResult.keys():\n","      if wordPair in topicWiseCount.keys():\n","        topicWiseCount[wordPair[1]] = topicWiseCount[wordPair[1]] + allResult[wordPair]\n","      else :\n","        topicWiseCount[wordPair[1]] = 1\n","\n","    laplaceSmoothingCalc = {}\n","    for word in uniqueWord:\n","      for topic in range(topicLength):\n","        \n","        wordPair = (word,topic)\n","        prevCount = 0\n","        if wordPair in allResult:\n","          prevCount = allResult[wordPair]\n","        probability = (prevCount+smFactor)/(float(topicWiseCount[topic])+smFactor*uniqueWordSize)\n","\n","        laplaceSmoothingCalc[wordPair] = probability\n","    \n","    return topicWiseCount,laplaceSmoothingCalc,uniqueWordSize\n","\n","  def predictNaive(word_list,laplaceSmoothingCalc,topicLength,topicWiseCount,uniqueWordSize,smFactor):\n","  \n","    maxProb = 0.0\n","    currTopic = 0\n","    allDistances = []\n","    for topic in range(topicLength):\n","      multiProb = math.log(1/float(topicLength))\n","      for word in word_list:\n","        wordPair = (word,topic)\n","        if wordPair in laplaceSmoothingCalc.keys():\n","          multiProb = multiProb + math.log(laplaceSmoothingCalc[wordPair])\n","        else :\n","          multiProb = multiProb + math.log((smFactor)/(topicWiseCount[topic]+smFactor*uniqueWordSize))\n","          #continue\n","      #print(multiProb)\n","      allDistances.append((topic,word,multiProb))\n","    allDistances.sort(key=lambda x: x[2],reverse=True)\n","    currTopic = allDistances[0][0]\n","    #print()\n","    return currTopic\n","  #print(trainDataSize)\n","  if type is 'val':\n","    for smFactor in smfactorList:\n","      print(\"smoothing factor constant :\",smFactor)\n","      topicWiseCount,laplaceSmoothingCalc,uniqueWordSize = trainNaive(allResult,topicLength,smFactor)\n","      #print(laplaceSmoothingCalc[(\"get\",1)])\n","      #print(uniqueWordSize)\n","      correctCount = 0\n","      totalCount = 0\n","      for key in topic_list:\n","        path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"+key+\".xml\"\n","        trainDataSize = trainDataSize\n","        validationDataSize = validationDataSize\n","        #print(trainDataSize)\n","        with open(path,'r',encoding='utf-8') as file:\n","          content = file.read()\n","          soup = bs(content)\n","          rowList = soup.findAll(\"row\")\n","          #correctCount = 0\n","          #print(trainDataSize)\n","          for row in rowList[trainDataSize:trainDataSize+validationDataSize+1]:\n","            #print(row.get('body'))\n","            #print()\n","            if len(row.get('body')) == 0:\n","              continue\n","            word_list = preprocessText(row.get('body'))\n","            if len(word_list) == 0:\n","              continue\n","            if predictNaive(word_list,laplaceSmoothingCalc,topicLength,topicWiseCount,uniqueWordSize,smFactor) is topic_list[key]:\n","              correctCount += 1\n","            totalCount += 1\n","      print((correctCount/float(totalCount))*100)\n","      print()\n","    return correctCount/float(totalCount)\n","\n","  elif type is 'test':\n","    for smFactor in smfactorList:\n","      print(\"smoothing factor constant :\",smFactor)\n","      topicWiseCount,laplaceSmoothingCalc,uniqueWordSize = trainNaive(allResult,topicLength,smFactor)\n","      #print(laplaceSmoothingCalc[(\"get\",1)])\n","      #print(uniqueWordSize)\n","      totalLoop = 50\n","      testSize = 10\n","      acc = []\n","      for loop in range(totalLoop):\n","        correctCount = 0\n","        totalCount = 0\n","        for key in topic_list:\n","          path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"+key+\".xml\"\n","          trainDataSize = trainDataSize\n","          #validationDataSize = validationDataSize\n","          #print(trainDataSize)\n","          with open(path,'r',encoding='utf-8') as file:\n","            content = file.read()\n","            soup = bs(content)\n","            rowList = soup.findAll(\"row\")\n","            #correctCount = 0\n","            #print(trainDataSize)\n","            for row in rowList[trainDataSize+validationDataSize+loop*testSize:trainDataSize++validationDataSize+loop*testSize+testSize]:\n","              #print(row.get('body'))\n","              #print()\n","              if len(row.get('body')) == 0:\n","                continue\n","              word_list = preprocessText(row.get('body'))\n","              if len(word_list) == 0:\n","                continue\n","              if predictNaive(word_list,laplaceSmoothingCalc,topicLength,topicWiseCount,uniqueWordSize,smFactor) is topic_list[key]:\n","                correctCount += 1\n","              totalCount += 1\n","\n","        acc.append(correctCount/float(totalCount))\n","    return acc\n","\n","train_path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"\n","#topic name reading\n","topic_number =0\n","topic_list = {}\n","with open(\"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/topics.txt\") as fp:\n","  lines = fp.readlines()\n","   \n","  for line in lines:\n","    line = line.strip()\n","    topic_list[line] = topic_number\n","    topic_number += 1\n","    '''\n","    if topic_number == 3 :\n","      break\n","    '''\n","fp.close()\n","print(topic_list)\n","topicLength = len(topic_list)\n","# train data and validation data \n","topicDataSize = 700\n","trainDataSize = 500\n","validationDataSize = 200\n","allResult = {}\n","trainDataStore = {}\n","validdataStore = {}\n","count = 0\n","\n","position_array = []\n","for i in range(0,topicDataSize):\n","  position_array.append(i)\n","\n","for topic in topic_list:\n","  \n","  with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","    content = file.read()\n","    soup = bs(content)\n","    rowList = soup.findAll(\"row\")\n","    count = 0\n","    for row in rowList[0:trainDataSize]:\n","      topicPair = (topic,position_array[count])\n","      if len(row.get('body')) == 0:\n","        continue;\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      for word in word_list:\n","        pairWord = (word,topic_list[topic])\n","        if pairWord in allResult:\n","          allResult[pairWord] += 1\n","        else:\n","          allResult[pairWord] = 1\n","      count += 1\n","      trainDataStore[topicPair] = np.array(word_list)\n","\n","    #validation data\n","    for i in range(trainDataSize,trainDataSize+validationDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      validdataStore[topicPair] = np.array(word_list)\n","\n","print(\"Document count :\",count)\n","\n","#smfactorList = [.001,.005,.01,.05,.08,0.1,0.3,0.5,0.8,1,10,20]\n","#accuracy = naiveBayes(allResult,trainDataSize,validationDataSize,topic_list,topicLength,smfactorList,'val')\n","smfactorList = [.05]\n","accuracy = naiveBayes(allResult,trainDataSize,validationDataSize,topic_list,topicLength,smfactorList,'test')\n","print(accuracy)\n","print(len(accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","{'Coffee': 0, 'Arduino': 1, 'Anime': 2, 'Astronomy': 3, 'Biology': 4, 'Chess': 5, 'Cooking': 6, 'Law': 7, 'Space': 8, 'Windows_Phone': 9, 'Wood_Working': 10}\n","Document count : 500\n","smoothing factor constant : 0.05\n","[0.8703703703703703, 0.8571428571428571, 0.9134615384615384, 0.875, 0.8380952380952381, 0.9056603773584906, 0.9259259259259259, 0.908256880733945, 0.8899082568807339, 0.8962264150943396, 0.8504672897196262, 0.912621359223301, 0.926605504587156, 0.8773584905660378, 0.8727272727272727, 0.8440366972477065, 0.8532110091743119, 0.8545454545454545, 0.8727272727272727, 0.8545454545454545, 0.8363636363636363, 0.8715596330275229, 0.8878504672897196, 0.9320388349514563, 0.8691588785046729, 0.8725490196078431, 0.8679245283018868, 0.8910891089108911, 0.9545454545454546, 0.8611111111111112, 0.8411214953271028, 0.8333333333333334, 0.8518518518518519, 0.8380952380952381, 0.8761904761904762, 0.8532110091743119, 0.8878504672897196, 0.8888888888888888, 0.8867924528301887, 0.9444444444444444, 0.8691588785046729, 0.8910891089108911, 0.8909090909090909, 0.8785046728971962, 0.897196261682243, 0.8363636363636363, 0.9142857142857143, 0.8909090909090909, 0.8715596330275229, 0.8679245283018868]\n","50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IPxpi8KrH-H_","executionInfo":{"status":"ok","timestamp":1602755388636,"user_tz":-360,"elapsed":9053589,"user":{"displayName":"Protik Bose Pranto","photoUrl":"","userId":"03376939609474751042"}},"outputId":"d9d32c90-a0a1-40d7-9e26-db53e3672153","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip3 install bs4\n","!pip install spacy\n","\n","import nltk\n","import math\n","import os\n","import fnmatch\n","import random\n","import numpy as np\n","from xml.etree import ElementTree\n","import re\n","import string\n","import spacy\n","from scipy.spatial import distance\n","from nltk.corpus import stopwords          \n","from nltk.stem import PorterStemmer        \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from bs4 import BeautifulSoup as bs\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","sp = spacy.load('en_core_web_sm')\n","spacy_stopwords = sp.Defaults.stop_words\n","\n","def preprocessText(text):\n","\n","  def makeTextLower(text):\n","    result = text.lower() \n","    return result \n","\n","  def MakeNumberRemove(text): \n","    #result = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', '', text)\n","    result = re.sub(r'[-+]?\\d+', '', text) \n","    return result \n","\n","  def MakeHTMLremove(text):\n","    cleancode = re.compile('<code>.*?</code>')\n","    cleanr = re.compile('<.*?>')\n","    cleanentity = re.compile('&.*;')\n","    cleantext = re.sub(cleancode, '', text)\n","    cleantext = re.sub(cleanr, ' ', cleantext)\n","    cleantext = re.sub(cleanentity, ' ', cleantext)  \n","    return cleantext\n","\n","  def makeRemoveExtraApace(text):\n","    result = \" \".join(text.split())  \n","    return result\n","\n","  def makeRemoveHyperLink(text):\n","    result = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","    return result  \n","\n","  def makeRemovePunctuation(text):\n","    result = text=text.translate((str.maketrans('','',string.punctuation)))\n","    return result\n","  \n","  def makeWordLemmatize(text):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = [lemmatizer.lemmatize(word,pos='v') for word in text] \n","    return lemmas\n","\n","  def removeStopWords(text):\n","    stopwords_english = stopwords.words('english')\n","    texts_clean = []\n","    #print(text)\n","    text = word_tokenize(text)\n","    for word in text: \n","      if word not in stopwords_english:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def removeSpacyStopWords(text,spacy_stopwords):\n","    texts_clean = []\n","    #print(text)\n","    for word in text: \n","      if word not in spacy_stopwords:  \n","          texts_clean.append(word)\n","    return texts_clean\n","\n","  def makeStemming(text):\n","    ps = PorterStemmer() \n","    #words = word_tokenize(text)\n","    lemmas = [ps.stem(word) for word in text] \n","    return lemmas  \n","\n","  result = text\n","  result=re.sub(r\"<.+?>\",'',result)\n","  result=re.sub(r'[^\\x00-\\x7F]+','',result)\n","  result = MakeHTMLremove(result)\n","  result = makeTextLower(result)\n","  result = MakeNumberRemove(result)\n","  result = makeRemoveHyperLink(result)\n","  result = makeRemovePunctuation(result)\n","  result = removeStopWords(result)\n","  result = removeSpacyStopWords(result,spacy_stopwords)\n","  result = makeWordLemmatize(result)\n","  result = makeStemming(result)\n","  return result\n","\n","def runTFIDF(datastore,validstore,topic_list,topicLength,type):\n","\n","  def numberDocsForWord(datastore,words):\n","    wordVal = {}\n","    value = list(datastore.values())\n","    for word in words:\n","      count = sum(word in e for e in value)    \n","      wordVal[word] = count\n","    return wordVal\n","\n","  def uniqueWordFunc(trainDataStore,testStore):\n","    uniqueWord = []\n","    for pair in trainDataStore:\n","      wordList = trainDataStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    wordval = numberDocsForWord(datastore,uniqueWord)\n","    for pair in testStore:\n","      wordList = testStore[pair]\n","      for word in wordList:\n","        if word not in uniqueWord:\n","          uniqueWord.append(word)\n","    return uniqueWord,wordval\n","\n","  def countData(datastore,validstore,uniqueWord):\n","    countDatastore = {}\n","    countDatastoreTest = {}\n","    \n","    for key in datastore:\n","      tempArray = list(datastore[key])\n","      wordDictA = []\n","      for word in uniqueWord:\n","        wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","      countDatastore[key] = wordDictA\n","      \n","    for key in validstore:\n","      tempArray = list(validstore[key])\n","      wordDictA = []\n","      for word in uniqueWord:\n","        wordDictA.append(tempArray.count(word)/float(len(tempArray)))\n","      countDatastoreTest[key] = wordDictA\n","    \n","    return countDatastore,countDatastoreTest\n","\n","  uniqueWord,wordval = uniqueWordFunc(datastore,validstore)    \n","  tfData,tfDataTest = countData(datastore,validstore,uniqueWord)\n","  print(len(uniqueWord))\n","\n","  def computeIDF(datastore,validstore,numberofDoc,dictionaryWords,wordval,tfData,tfDataTest):\n","    \n","    idfDict = {}\n","    idfDict = dict.fromkeys(set(dictionaryWords), 0)\n","    \n","    for key in datastore:\n","      tempArray = datastore[key]\n","      temp = []\n","      for word in dictionaryWords:\n","        if word in tempArray:\n","          temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","        else :\n","          temp.append(0.0)\n","      tfData[key] = list(np.multiply(temp,tfData[key]))\n","\n","    for key in validstore:\n","      tempArray = validstore[key]\n","      temp = []\n","      for word in dictionaryWords:\n","        if word in tempArray and word in wordval:\n","          temp.append(math.log10(numberofDoc / (float(wordval[word]) + 1)))\n","        else :\n","          temp.append(0.0)\n","      tfDataTest[key] = list(np.multiply(temp,tfDataTest[key]))\n","        \n","    return tfData,tfDataTest\n","\n","  numberofDoc = len(datastore)\n","  #print(numberofDoc)\n","\n","  idfData = {}\n","  tfidfDataArray,tfidfDataTestArray = computeIDF(datastore,validstore,numberofDoc,uniqueWord,wordval,tfData,tfDataTest)\n","  \n","  def cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval):\n","    listVal1 = np.sqrt(np.sum(np.square(testingBinArrCS),axis=1))\n","    listVal2 = np.sqrt(np.sum(np.square(tfidfDataTestArray[keyval])))\n","    dotResult = np.dot(testingBinArrCS,tfidfDataTestArray[keyval])\n","    finalResult = np.divide(dotResult,np.dot(listVal1,listVal2))\n","    finalResult, headerCS = (list(t) for t in zip(*sorted(zip(finalResult, headerCS),reverse=True)))\n","    return headerCS\n","\n","  headerCS = list(tfidfDataArray.keys())\n","  testingBinArrCS = np.array(list(tfidfDataArray.values()))\n","\n","  allBinDisctance = {}\n","  for keyval in tfidfDataTestArray:\n","    predicted = cosineSimilarity(headerCS,testingBinArrCS,tfidfDataTestArray,keyval)\n","    allBinDisctance[keyval] = predicted\n","\n","  if type is 'val':\n","    n_neighbors = [1,3,5]\n","  elif type is 'test':\n","    n_neighbors = [5]\n","\n","  for neighbor in n_neighbors:\n","    print(\"Neighbor :\",neighbor)\n","    totalCount = 0\n","    correctCount = 0\n","    for keyval in tfidfDataTestArray:\n","      predArray = allBinDisctance[keyval]\n","      voting_count = {}\n","      for val in predArray[:neighbor]:\n","        if val[0] in voting_count:\n","          voting_count[val[0]] += 1\n","        else :\n","          voting_count[val[0]] = 1\n","\n","      prediction = max(voting_count, key=voting_count.get)\n","      if prediction == keyval[0]:\n","        correctCount += 1\n","      totalCount += 1\n","      \n","    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n","  print()\n","  return correctCount/float(totalCount)\n","\n","\n","\n","train_path = \"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/Training/\"\n","#topic name reading\n","topic_number =0\n","topic_list = {}\n","with open(\"/content/drive/My Drive/Machine Learning Algorithm/kNN/Data/topics.txt\") as fp:\n","  lines = fp.readlines()\n","   \n","  for line in lines:\n","    line = line.strip()\n","    topic_list[line] = topic_number\n","    topic_number += 1\n","    '''\n","    if topic_number == 3 :\n","      break\n","    '''\n","fp.close()\n","print(topic_list)\n","topicLength = len(topic_list)\n","# train data and validation data \n","topicDataSize = 700\n","trainDataSize = 500\n","validationDataSize = 200\n","allResult = {}\n","trainDataStore = {}\n","validdataStore = {}\n","count = 0\n","\n","position_array = []\n","for i in range(0,topicDataSize):\n","  position_array.append(i)\n","\n","for topic in topic_list:\n","  \n","  with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","    content = file.read()\n","    soup = bs(content)\n","    rowList = soup.findAll(\"row\")\n","    count = 0\n","    for row in rowList[0:trainDataSize]:\n","      topicPair = (topic,position_array[count])\n","      if len(row.get('body')) == 0:\n","        continue;\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      for word in word_list:\n","        pairWord = (word,topic_list[topic])\n","        if pairWord in allResult:\n","          allResult[pairWord] += 1\n","        else:\n","          allResult[pairWord] = 1\n","      count += 1\n","      trainDataStore[topicPair] = np.array(word_list)\n","\n","    #validation data\n","    for i in range(trainDataSize,trainDataSize+validationDataSize):\n","      topicPair = (topic,position_array[i])\n","      row = rowList[position_array[i]]\n","      if len(row.get('body')) == 0:\n","        continue\n","      word_list = preprocessText(row.get('body'))\n","      if len(word_list) == 0:\n","        continue\n","      validdataStore[topicPair] = np.array(word_list)\n","\n","print(\"Document count :\",count)\n","\n","totalLoop = 50\n","acc = []\n","for loop in range(20,30):\n","  testSize = 10\n","  position_array = []\n","  for i in range(topicDataSize+loop*testSize,topicDataSize+loop*testSize+testSize):\n","    position_array.append(i)\n","  testDatastore = {}\n","\n","  for topic in topic_list:\n","    with open(train_path+topic+\".xml\",'r',encoding='utf-8') as file:\n","      content = file.read()\n","      soup = bs(content)\n","      rowList = soup.findAll(\"row\")\n","      \n","      for i in range(len(position_array)):\n","        topicPair = (topic,position_array[i])\n","        row = rowList[position_array[i]]\n","        if len(row.get('body')) == 0:\n","          continue\n","        word_list = preprocessText(row.get('body'))\n","        if len(word_list) == 0:\n","          continue\n","        testDatastore[topicPair] = np.array(word_list) \n","\n","  accuracy = runTFIDF(trainDataStore,testDatastore,topic_list,topicLength,'test')\n","  acc.append(accuracy)\n","\n","print(acc)\n","print(len(acc))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","{'Coffee': 0, 'Arduino': 1, 'Anime': 2, 'Astronomy': 3, 'Biology': 4, 'Chess': 5, 'Cooking': 6, 'Law': 7, 'Space': 8, 'Windows_Phone': 9, 'Wood_Working': 10}\n","Document count : 500\n","19512\n","Neighbor : 5\n","Total Correct Count:  87  Total Wrong Count:  23  Accuracy:  79.0909090909091\n","\n","19499\n","Neighbor : 5\n","Total Correct Count:  86  Total Wrong Count:  23  Accuracy:  78.89908256880734\n","\n","19549\n","Neighbor : 5\n","Total Correct Count:  88  Total Wrong Count:  19  Accuracy:  82.24299065420561\n","\n","19514\n","Neighbor : 5\n","Total Correct Count:  84  Total Wrong Count:  19  Accuracy:  81.55339805825243\n","\n","19583\n","Neighbor : 5\n","Total Correct Count:  84  Total Wrong Count:  23  Accuracy:  78.50467289719626\n","\n","19468\n","Neighbor : 5\n","Total Correct Count:  80  Total Wrong Count:  22  Accuracy:  78.43137254901961\n","\n","19436\n","Neighbor : 5\n","Total Correct Count:  87  Total Wrong Count:  19  Accuracy:  82.0754716981132\n","\n","19438\n","Neighbor : 5\n","Total Correct Count:  88  Total Wrong Count:  13  Accuracy:  87.12871287128714\n","\n","19489\n","Neighbor : 5\n","Total Correct Count:  91  Total Wrong Count:  19  Accuracy:  82.72727272727273\n","\n","19482\n","Neighbor : 5\n","Total Correct Count:  93  Total Wrong Count:  15  Accuracy:  86.11111111111111\n","\n","[0.7909090909090909, 0.7889908256880734, 0.822429906542056, 0.8155339805825242, 0.7850467289719626, 0.7843137254901961, 0.8207547169811321, 0.8712871287128713, 0.8272727272727273, 0.8611111111111112]\n","10\n"],"name":"stdout"}]}]}